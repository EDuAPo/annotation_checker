"""
æ ‡æ³¨æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æµæ°´çº¿
========================

å®Œæ•´æµç¨‹ï¼š
1. æ ¹æ®æœ¬åœ° JSON æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶åï¼Œä» DataWeave å¹³å°ä¸‹è½½å¯¹åº”çš„ ZIP æ–‡ä»¶åˆ°æœ¬åœ°
2. ä¸Šä¼  ZIP æ–‡ä»¶åˆ°æœåŠ¡å™¨ /data01/rere_zips
3. åœ¨æœåŠ¡å™¨ä¸Šè§£å‹ ZIPï¼ŒåŒ¹é…æ›¿æ¢å¯¹åº”çš„æ ‡æ³¨ JSON æ–‡ä»¶ï¼Œè°ƒæ•´ç›®å½•ç»“æ„
4. ä¸‹è½½å¤„ç†åçš„æ•°æ®åˆ°æœ¬åœ°ï¼Œè¿›è¡Œæ ‡æ³¨è´¨é‡æ£€æŸ¥
5. å°†æ£€æŸ¥é€šè¿‡çš„æ•°æ®ç§»åŠ¨åˆ°æœåŠ¡å™¨æœ€ç»ˆç›®å½•

ä½¿ç”¨æ–¹æ³•ï¼š
    python pipeline.py --json_dir /path/to/jsons --step all
    python pipeline.py --json_dir /path/to/jsons --step download
    python pipeline.py --json_dir /path/to/jsons --step check
"""

import os
import sys
import yaml
import argparse
import logging
import tempfile
import shutil
import copy
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ================= é…ç½®åŒºåŸŸ =================
# DataWeave API é…ç½®
API_BASE_URL = "https://dataweave.enableai.cn/api/v4"
API_URL = f"{API_BASE_URL}/file/url"
# Cloudreve v4 ç™»å½• API
LOGIN_URL = f"{API_BASE_URL}/session/token"

# DataWeave ç™»å½•å‡­æ® (ç”¨äºè‡ªåŠ¨è·å– Token)
# å¦‚æœè®¾ç½®äº†ç”¨æˆ·åå¯†ç ï¼Œä¼šè‡ªåŠ¨ç™»å½•è·å– Token
DATAWEAVE_USERNAME = "dongshucai@126.com"  # å¡«å†™ä½ çš„ç”¨æˆ·å
DATAWEAVE_PASSWORD = "dongshucai"  # å¡«å†™ä½ çš„å¯†ç 

# å¤šä¸ªå¯èƒ½çš„è·¯å¾„æ¨¡æ¿ (æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼Œä¼šä¾æ¬¡å°è¯•ç›´åˆ°æ‰¾åˆ°æ–‡ä»¶)
DATAWEAVE_PATH_TEMPLATES = [
    "dataweave://my/TO_RERE/ç›²åŒºæ•°æ®/{filename}",
    "dataweave://my/TO_RERE/7Lidar_data/{filename}",
    "dataweave://my/TO_RERE/å·²ä¸Šä¼ å¹³å°/{filename}",
    "dataweave://my/TO_RERE/æœªä¸Šä¼ å¹³å°/{filename}",
    "dataweave://my/TO_RERE/å‰”é™¤éå…³é”®å¸§&é‡æ–°ä¸Šä¼ /{filename}",
    "dataweave://my/TO_RERE/12-9/{filename}",
]
# å¤‡ç”¨ Token (å¦‚æœè‡ªåŠ¨ç™»å½•å¤±è´¥ï¼Œä¼šä½¿ç”¨æ­¤ Token)
AUTH_TOKEN = "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0b2tlbl90eXBlIjoiYWNjZXNzIiwic3ViIjoidjRCUWlhIiwiZXhwIjoxNzY3Njc3NDMzLCJuYmYiOjE3Njc2NzM4MzN9.F0C1ZkAQxr4uAGVBRIpIMXFJwHW9Ke1x-KshxLMgCs8"

# æœåŠ¡å™¨é…ç½®
SERVER_IP = "222.223.112.212"
SERVER_USER = "user"
SERVER_ZIP_DIR = "/data01/rere_zips"                    # ä¸Šä¼  ZIP çš„ä¸´æ—¶ç›®å½•
SERVER_PROCESS_DIR = "/data01/processing"  # å¤„ç†ä¸­çš„æ•°æ®ç›®å½•
SERVER_FINAL_DIR = "/data01/dataset/scenesnew"         # æ£€æŸ¥é€šè¿‡åçš„æœ€ç»ˆç›®å½•

# å¤„ç†å®Œæˆåå¯¹åŸå§‹ ZIP çš„æ“ä½œæ–¹å¼
# "rename": é‡å‘½åä¸º processed_xxx.zip (é»˜è®¤ï¼Œæ ‡è®°å·²å¤„ç†)
# "keep": ä¿ç•™åŸå§‹æ–‡ä»¶ä¸å˜
# "delete": åˆ é™¤åŸå§‹ ZIP æ–‡ä»¶
ZIP_AFTER_PROCESS = "rename"

# æœ¬åœ°ä¸´æ—¶ç›®å½• (ç”¨äºä¸‹è½½ ZIP å’Œæ£€æŸ¥æ•°æ®)
LOCAL_TEMP_DIR = "/media/zgw/T7/test_pipeline_downzips/"

# æ˜¯å¦å°† JSON é‡å‘½åä¸º annotations.json
RENAME_JSON = True

# å¤šçº¿ç¨‹é…ç½®
MAX_WORKERS = 3  # å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•° (å»ºè®® 2-4ï¼Œå¤ªå¤šä¼šå ç”¨æœåŠ¡å™¨èµ„æº)

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_PATH = "configs/user_config.yaml"
# ===========================================

import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç®€æ´çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ç¦ç”¨ paramiko çš„è¯¦ç»†æ—¥å¿—
logging.getLogger("paramiko").setLevel(logging.WARNING)

# çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤å…±äº«èµ„æº
results_lock = threading.Lock()

# è¿›åº¦æ˜¾ç¤ºå·¥å…·
class ProgressTracker:
    """ç®€æ´çš„è¿›åº¦è¿½è¸ªå™¨"""
    def __init__(self, total: int, title: str = "å¤„ç†è¿›åº¦"):
        self.total = total
        self.completed = 0
        self.success = 0
        self.failed = 0
        self.title = title
        self.lock = threading.Lock()
        self.start_time = datetime.now()
    
    def update(self, success: bool = True, name: str = ""):
        with self.lock:
            self.completed += 1
            if success:
                self.success += 1
            else:
                self.failed += 1
            self._display(name, success)
    
    def _display(self, name: str, success: bool):
        percent = self.completed / self.total * 100 if self.total > 0 else 0
        width = 25
        filled = int(width * self.completed / self.total) if self.total > 0 else 0
        bar = 'â”' * filled + 'â•¸' + 'â”€' * (width - filled - 1) if filled < width else 'â”' * width
        
        status = "âœ“" if success else "âœ—"
        elapsed = (datetime.now() - self.start_time).seconds
        
        # æ¸…é™¤å½“å‰è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
        sys.stdout.write(f'\r\033[K')
        sys.stdout.write(f'[{bar}] {self.completed}/{self.total} ({percent:.0f}%) â”‚ {status} {name[:30]:<30}')
        sys.stdout.flush()
        
        if self.completed >= self.total:
            print()  # å®Œæˆåæ¢è¡Œ
    
    def summary(self):
        elapsed = (datetime.now() - self.start_time).seconds
        mins, secs = divmod(elapsed, 60)
        print(f"\n{'â”€'*50}")
        print(f"  ğŸ“Š {self.title} å®Œæˆ")
        print(f"  âœ“ æˆåŠŸ: {self.success}  âœ— å¤±è´¥: {self.failed}  â± è€—æ—¶: {mins}åˆ†{secs}ç§’")
        print(f"{'â”€'*50}")


class AnnotationPipeline:
    """æ ‡æ³¨æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, json_dir: str, local_zip_dir: str = None):
        self.json_dir = Path(json_dir)
        self.local_zip_dir = Path(local_zip_dir) if local_zip_dir else Path(LOCAL_TEMP_DIR) / "zips"
        self.local_check_dir = Path(LOCAL_TEMP_DIR) / "check_data"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.local_zip_dir.mkdir(parents=True, exist_ok=True)
        self.local_check_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è½½æ–‡ä»¶ï¼ˆ.tmp ä¸´æ—¶æ–‡ä»¶ï¼‰
        self._cleanup_incomplete_downloads()
        
        # SSH è¿æ¥
        self.ssh = None
        self.sftp = None
        
        # å¤„ç†ç»“æœè·Ÿè¸ª
        self.results = {
            'downloaded': [],
            'skipped_server_exists': [],  # æœåŠ¡å™¨ä¸Šå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            'uploaded': [],
            'processed': [],
            'check_passed': [],
            'check_failed': [],
            'moved_to_final': []
        }
        
        # é”™è¯¯è¿½è¸ª (ç”¨äºè¿½æº¯å¤±è´¥åŸå› )
        self.errors = {}  # {stem: [(step, error_msg), ...]}
        self.errors_lock = threading.Lock()
        
        # Token ç®¡ç†
        self._token = None
        self._token_time = None
        self._token_lock = threading.Lock()
        self._token_max_age = 50 * 60  # Token æœ‰æ•ˆæœŸ 50 åˆ†é’Ÿ (æœåŠ¡ç«¯1å°æ—¶è¿‡æœŸ)
    
    def _cleanup_incomplete_downloads(self):
        """æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è½½æ–‡ä»¶ï¼ˆ.tmp ä¸´æ—¶æ–‡ä»¶ï¼‰"""
        tmp_files = list(self.local_zip_dir.glob("*.tmp"))
        if tmp_files:
            logger.info(f"å‘ç° {len(tmp_files)} ä¸ªæœªå®Œæˆçš„ä¸‹è½½ï¼Œæ­£åœ¨æ¸…ç†...")
            for tmp_file in tmp_files:
                try:
                    tmp_file.unlink()
                    logger.info(f"  å·²åˆ é™¤: {tmp_file.name}")
                except Exception as e:
                    logger.warning(f"  åˆ é™¤å¤±è´¥ {tmp_file.name}: {e}")
    
    def _is_valid_zip(self, zip_path: Path) -> bool:
        """æ£€æŸ¥ ZIP æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆï¼ˆå­˜åœ¨ã€éç©ºã€å¯è¯»ï¼‰"""
        if not zip_path.exists():
            return False
        if zip_path.stat().st_size == 0:
            return False
        # å¿«é€Ÿæ£€æŸ¥ ZIP æ–‡ä»¶å¤´
        try:
            with open(zip_path, 'rb') as f:
                header = f.read(4)
                # ZIP æ–‡ä»¶ä»¥ PK\x03\x04 å¼€å¤´
                return header[:2] == b'PK'
        except:
            return False
    
    def _connect_server(self):
        """è¿æ¥è¿œç¨‹æœåŠ¡å™¨"""
        if self.ssh is not None:
            return True
        try:
            import paramiko
            self.ssh = paramiko.SSHClient()
            self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            logger.info(f"æ­£åœ¨è¿æ¥æœåŠ¡å™¨ {SERVER_IP}...")
            self.ssh.connect(SERVER_IP, username=SERVER_USER, timeout=10)
            self.sftp = self.ssh.open_sftp()
            logger.info("æœåŠ¡å™¨è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"è¿æ¥æœåŠ¡å™¨å¤±è´¥: {e}")
            return False
    
    def _close_server(self):
        """å…³é—­æœåŠ¡å™¨è¿æ¥"""
        if self.sftp:
            self.sftp.close()
        if self.ssh:
            self.ssh.close()
        self.ssh = None
        self.sftp = None
    
    def _exec_remote(self, cmd: str) -> tuple:
        """æ‰§è¡Œè¿œç¨‹å‘½ä»¤"""
        stdin, stdout, stderr = self.ssh.exec_command(cmd)
        exit_status = stdout.channel.recv_exit_status()
        out = stdout.read().decode().strip()
        err = stderr.read().decode().strip()
        return exit_status, out, err
    
    def _exec_remote_thread(self, ssh, cmd: str, timeout: int = 60) -> tuple:
        """çº¿ç¨‹ä¸­æ‰§è¡Œè¿œç¨‹å‘½ä»¤ (ä½¿ç”¨ä¼ å…¥çš„ ssh è¿æ¥)ï¼Œå¸¦è¶…æ—¶æ§åˆ¶"""
        try:
            stdin, stdout, stderr = ssh.exec_command(cmd, timeout=timeout)
            exit_status = stdout.channel.recv_exit_status()
            out = stdout.read().decode().strip()
            err = stderr.read().decode().strip()
            return exit_status, out, err
        except Exception as e:
            return -1, "", str(e)
    
    def _get_dataweave_token(self, force_refresh: bool = False) -> str:
        """è‡ªåŠ¨ç™»å½• DataWeave (Cloudreve v4) è·å– Token (çº¿ç¨‹å®‰å…¨ï¼Œæ”¯æŒè‡ªåŠ¨åˆ·æ–°)"""
        import requests
        import time
        
        with self._token_lock:
            # æ£€æŸ¥ç¼“å­˜çš„ Token æ˜¯å¦æœ‰æ•ˆ
            if not force_refresh and self._token and self._token_time:
                elapsed = time.time() - self._token_time
                if elapsed < self._token_max_age:
                    return self._token
                else:
                    logger.info("Token å³å°†è¿‡æœŸï¼Œè‡ªåŠ¨åˆ·æ–°...")
            
            # å¦‚æœæ²¡æœ‰é…ç½®ç”¨æˆ·åå¯†ç ï¼Œä½¿ç”¨å¤‡ç”¨ Token
            if not DATAWEAVE_USERNAME or not DATAWEAVE_PASSWORD:
                return AUTH_TOKEN
            
            # é‡è¯•è·å– Token
            for attempt in range(3):
                try:
                    login_data = {
                        "email": DATAWEAVE_USERNAME,
                        "password": DATAWEAVE_PASSWORD
                    }
                    headers = {
                        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
                        "Content-Type": "application/json",
                        "Accept": "application/json",
                        "Origin": "https://dataweave.enableai.cn",
                        "Referer": "https://dataweave.enableai.cn/session",
                    }
                    
                    r = requests.post(LOGIN_URL, json=login_data, headers=headers, timeout=15)
                    data = r.json()
                    
                    if data.get("code") == 0:
                        token_data = data.get("data", {}).get("token", {})
                        access_token = token_data.get("access_token")
                        
                        if access_token:
                            self._token = f"Bearer {access_token}"
                            self._token_time = time.time()
                            if not force_refresh:
                                print("  ğŸ”‘ Token è·å–æˆåŠŸ")
                            else:
                                logger.info("ğŸ”‘ Token åˆ·æ–°æˆåŠŸ")
                            return self._token
                
                except Exception as e:
                    if attempt < 2:
                        time.sleep(1)
                        continue
            
            print("  âš  ä½¿ç”¨å¤‡ç”¨ Token")
            return AUTH_TOKEN
    
    # ==================== æ­¥éª¤ 1: ä¸‹è½½ ZIP ====================
    def step1_download_zips(self):
        """ä» DataWeave ä¸‹è½½ ZIP æ–‡ä»¶"""
        import requests
        
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 1: ä» DataWeave ä¸‹è½½ ZIP æ–‡ä»¶")
        logger.info("=" * 60)
        
        json_files = list(self.json_dir.glob("*.json"))
        if not json_files:
            logger.warning(f"æœªåœ¨ {self.json_dir} æ‰¾åˆ° JSON æ–‡ä»¶")
            return
        
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶ï¼Œå‡†å¤‡ä¸‹è½½å¯¹åº”çš„ ZIP")
        
        # è¿æ¥æœåŠ¡å™¨è·å–å·²å­˜åœ¨çš„ZIPæ–‡ä»¶åˆ—è¡¨
        # å­˜å‚¨åŸå§‹æ–‡ä»¶åï¼ˆå»æ‰ processed_ å‰ç¼€ï¼‰ï¼Œä¾¿äºç»Ÿä¸€åŒ¹é…
        server_zip_originals = set()
        if self._connect_server():
            status, out, err = self._exec_remote(f"ls {SERVER_ZIP_DIR}/*.zip 2>/dev/null || true")
            if out:
                for line in out.splitlines():
                    name = Path(line.strip()).name
                    # å»æ‰ processed_ å‰ç¼€ï¼Œç»Ÿä¸€å­˜å‚¨åŸå§‹åç§°
                    if name.startswith("processed_"):
                        original_name = name[len("processed_"):]
                        server_zip_originals.add(original_name)
                    else:
                        server_zip_originals.add(name)
            logger.info(f"æœåŠ¡å™¨ä¸Šå·²æœ‰ {len(server_zip_originals)} ä¸ª ZIP æ–‡ä»¶ (å«å·²å¤„ç†)")
        
        # è‡ªåŠ¨è·å– Token
        auth_token = self._get_dataweave_token()
        
        headers = {
            "User-Agent": "Mozilla/5.0",
            "Content-Type": "application/json",
            "Authorization": auth_token,
        }
        
        for i, json_file in enumerate(json_files):
            stem = json_file.stem
            zip_name = f"{stem}.zip"
            target_file = self.local_zip_dir / zip_name
            
            logger.info(f"[{i+1}/{len(json_files)}] å¤„ç†: {stem}")
            
            # æ£€æŸ¥æœåŠ¡å™¨ä¸Šæ˜¯å¦å·²å­˜åœ¨ (ç»Ÿä¸€æ¯”è¾ƒåŸå§‹æ–‡ä»¶å)
            if zip_name in server_zip_originals:
                logger.info(f"    æœåŠ¡å™¨ä¸Šå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
                self.results['skipped_server_exists'].append(stem)
                continue
            
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²å­˜åœ¨ä¸”å®Œæ•´
            if self._is_valid_zip(target_file):
                logger.info(f"    æœ¬åœ°æ–‡ä»¶å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½")
                self.results['downloaded'].append(stem)
                continue
            
            try:
                # åœ¨å¤šä¸ªè·¯å¾„æ¨¡æ¿ä¸­æŸ¥æ‰¾æ–‡ä»¶
                real_url = None
                found_path = None
                
                for path_template in DATAWEAVE_PATH_TEMPLATES:
                    dw_path = path_template.format(filename=zip_name)
                    payload = {"uris": [dw_path]}
                    
                    r = requests.post(API_URL, json=payload, headers=headers, timeout=15)
                    r.raise_for_status()
                    data = r.json()
                    
                    # æ£€æŸ¥è®¤è¯é”™è¯¯
                    if data.get("code") != 0:
                        msg = data.get("msg", "")
                        if "Login required" in msg or data.get("code") == 401:
                            logger.critical("!!! Token å·²è¿‡æœŸï¼Œè¯·æ›´æ–° AUTH_TOKEN !!!")
                            return
                        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„
                        continue
                    
                    # è§£æ URL
                    url_data = data.get("data", {})
                    if isinstance(url_data, dict) and "urls" in url_data:
                        urls_list = url_data["urls"]
                        if urls_list and isinstance(urls_list[0], dict):
                            url = urls_list[0].get("url")
                            if url:
                                real_url = url
                                found_path = path_template.split("/")[-2]  # æå–å­ç›®å½•å
                                break
                
                if not real_url:
                    logger.warning(f"    åœ¨æ‰€æœ‰è·¯å¾„ä¸­å‡æœªæ‰¾åˆ°æ–‡ä»¶")
                    continue
                
                logger.info(f"    æ‰¾åˆ°æ–‡ä»¶ï¼Œè·¯å¾„: {found_path}")
                
                # ä¸‹è½½æ–‡ä»¶
                logger.info(f"    å¼€å§‹ä¸‹è½½...")
                download_headers = {"User-Agent": headers["User-Agent"]}
                with requests.get(real_url, headers=download_headers, stream=True, timeout=300) as r:
                    r.raise_for_status()
                    total_size = int(r.headers.get('content-length', 0))
                    with open(target_file, 'wb') as f:
                        downloaded = 0
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                if total_size > 0:
                                    percent = (downloaded / total_size) * 100
                                    sys.stdout.write(f"\r    ä¸‹è½½è¿›åº¦: {percent:.1f}%")
                                    sys.stdout.flush()
                    print()
                
                self.results['downloaded'].append(stem)
                logger.info(f"    ä¸‹è½½å®Œæˆ")
                
            except Exception as e:
                logger.error(f"    ä¸‹è½½å¤±è´¥: {e}")
                if target_file.exists():
                    target_file.unlink()
        
        logger.info(f"ä¸‹è½½é˜¶æ®µå®Œæˆ: æ–°ä¸‹è½½ {len(self.results['downloaded'])} ä¸ª, è·³è¿‡(æœåŠ¡å™¨å·²æœ‰) {len(self.results['skipped_server_exists'])} ä¸ª")
    
    # ==================== æ­¥éª¤ 2: ä¸Šä¼  ZIP åˆ°æœåŠ¡å™¨ ====================
    def step2_upload_zips(self):
        """ä¸Šä¼  ZIP æ–‡ä»¶åˆ°æœåŠ¡å™¨"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 2: ä¸Šä¼  ZIP æ–‡ä»¶åˆ°æœåŠ¡å™¨")
        logger.info("=" * 60)
        
        if not self._connect_server():
            return
        
        # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
        self._exec_remote(f"mkdir -p {SERVER_ZIP_DIR}")
        
        zip_files = list(self.local_zip_dir.glob("*.zip"))
        if not zip_files:
            logger.warning("æ²¡æœ‰æ‰¾åˆ° ZIP æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            return
        
        logger.info(f"å‡†å¤‡ä¸Šä¼  {len(zip_files)} ä¸ª ZIP æ–‡ä»¶")
        
        for i, zip_file in enumerate(zip_files):
            remote_path = f"{SERVER_ZIP_DIR}/{zip_file.name}"
            file_size_mb = zip_file.stat().st_size / (1024 * 1024)
            
            logger.info(f"[{i+1}/{len(zip_files)}] ä¸Šä¼ : {zip_file.name} ({file_size_mb:.1f} MB)")
            
            try:
                # æ£€æŸ¥è¿œç¨‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                try:
                    remote_stat = self.sftp.stat(remote_path)
                    if remote_stat.st_size == zip_file.stat().st_size:
                        logger.info(f"    æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        self.results['uploaded'].append(zip_file.stem)
                        continue
                except FileNotFoundError:
                    pass
                
                # ä¸Šä¼ 
                self.sftp.put(str(zip_file), remote_path)
                self.results['uploaded'].append(zip_file.stem)
                logger.info(f"    ä¸Šä¼ å®Œæˆ")
                
            except Exception as e:
                logger.error(f"    ä¸Šä¼ å¤±è´¥: {e}")
        
        logger.info(f"ä¸Šä¼ å®Œæˆ: {len(self.results['uploaded'])} ä¸ªæ–‡ä»¶")
    
    # ==================== æ­¥éª¤ 3: æœåŠ¡å™¨ç«¯è§£å‹å¤„ç† ====================
    def step3_process_on_server(self):
        """åœ¨æœåŠ¡å™¨ä¸Šè§£å‹å¹¶å¤„ç† ZIP æ–‡ä»¶"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 3: æœåŠ¡å™¨ç«¯è§£å‹å¤„ç†")
        logger.info("=" * 60)
        
        if not self._connect_server():
            return
        
        # ç¡®ä¿å¤„ç†ç›®å½•å­˜åœ¨
        self._exec_remote(f"mkdir -p {SERVER_PROCESS_DIR}")
        
        # éƒ¨ç½² worker è„šæœ¬
        self._deploy_worker_script()
        
        # è·å–æœåŠ¡å™¨ä¸Šçš„ ZIP æ–‡ä»¶
        status, out, err = self._exec_remote(f"ls {SERVER_ZIP_DIR}/*.zip 2>/dev/null || true")
        if not out:
            logger.warning("æœåŠ¡å™¨ä¸Šæ²¡æœ‰æ‰¾åˆ° ZIP æ–‡ä»¶")
            return
        
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„ ZIP (ä»¥ processed_ å¼€å¤´çš„)
        all_zips = [Path(f.strip()) for f in out.splitlines() if not Path(f.strip()).name.startswith("processed_")]
        
        # å¦‚æœé…ç½®ä¸º keep æ¨¡å¼ï¼Œéœ€è¦æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨æ¥åˆ¤æ–­æ˜¯å¦å·²å¤„ç†
        remote_zips = []
        if ZIP_AFTER_PROCESS == "keep":
            for zip_path in all_zips:
                # æ£€æŸ¥å¤„ç†è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨
                check_dir = f"{SERVER_PROCESS_DIR}/{zip_path.stem}"
                status, _, _ = self._exec_remote(f"test -d '{check_dir}' && echo exists")
                if status != 0:
                    remote_zips.append(zip_path)
                else:
                    logger.info(f"è·³è¿‡å·²å¤„ç†: {zip_path.name} (è¾“å‡ºç›®å½•å·²å­˜åœ¨)")
        else:
            remote_zips = all_zips
        
        logger.info(f"æœåŠ¡å™¨ä¸Šå‘ç° {len(remote_zips)} ä¸ªå¾…å¤„ç† ZIP æ–‡ä»¶")
        
        for i, zip_path in enumerate(remote_zips):
            zip_stem = zip_path.stem
            logger.info(f"[{i+1}/{len(remote_zips)}] å¤„ç†: {zip_path.name}")
            
            # æŸ¥æ‰¾å¯¹åº”çš„æœ¬åœ° JSON
            local_json = self._find_local_json(zip_stem)
            if not local_json:
                logger.warning(f"    è·³è¿‡: æœªæ‰¾åˆ°å¯¹åº”çš„ JSON æ–‡ä»¶")
                continue
            
            # ä¸Šä¼  JSON åˆ°æœåŠ¡å™¨ä¸´æ—¶ä½ç½®
            remote_json_temp = f"/tmp/{local_json.name}"
            try:
                self.sftp.put(str(local_json), remote_json_temp)
                logger.info(f"    å·²ä¸Šä¼  JSON: {local_json.name}")
            except Exception as e:
                logger.error(f"    ä¸Šä¼  JSON å¤±è´¥: {e}")
                continue
            
            # æ‰§è¡Œè¿œç¨‹å¤„ç†è„šæœ¬
            cmd = f"python3 /tmp/zip_worker.py --zip '{zip_path}' --json '{remote_json_temp}' --out '{SERVER_PROCESS_DIR}' --rename_json '{RENAME_JSON}'"
            status, out, err = self._exec_remote(cmd)
            
            if status == 0:
                logger.info(f"    å¤„ç†æˆåŠŸ")
                # æ ¹æ®é…ç½®å¤„ç†åŸå§‹ ZIP æ–‡ä»¶
                if ZIP_AFTER_PROCESS == "rename":
                    new_name = zip_path.parent / f"processed_{zip_path.name}"
                    self._exec_remote(f"mv '{zip_path}' '{new_name}'")
                    logger.info(f"    åŸå§‹ ZIP å·²é‡å‘½åä¸º: processed_{zip_path.name}")
                elif ZIP_AFTER_PROCESS == "delete":
                    self._exec_remote(f"rm '{zip_path}'")
                    logger.info(f"    åŸå§‹ ZIP å·²åˆ é™¤")
                else:  # keep
                    logger.info(f"    åŸå§‹ ZIP å·²ä¿ç•™")
                self.results['processed'].append(zip_stem)
            else:
                logger.error(f"    å¤„ç†å¤±è´¥: {err}")
        
        logger.info(f"å¤„ç†å®Œæˆ: {len(self.results['processed'])} ä¸ªæ–‡ä»¶")
    
    def _find_local_json(self, zip_stem: str) -> Path:
        """æŸ¥æ‰¾æœ¬åœ°å¯¹åº”çš„ JSON æ–‡ä»¶"""
        # ç²¾ç¡®åŒ¹é…
        exact = self.json_dir / f"{zip_stem}.json"
        if exact.exists():
            return exact
        
        # æ¨¡ç³ŠåŒ¹é…
        for f in self.json_dir.glob("*.json"):
            if zip_stem in f.stem:
                return f
        return None
    
    def _deploy_worker_script(self):
        """éƒ¨ç½²è¿œç¨‹ worker è„šæœ¬"""
        worker_code = '''
import os, sys, shutil, zipfile, argparse
from pathlib import Path

def find_data_root(extract_dir):
    required = ["camera_cam_3M_front", "combined_scales", "ins.json", "sample.json"]
    for root, dirs, files in os.walk(extract_dir):
        count = sum(1 for name in dirs + files if name in required)
        if count >= 2:
            return Path(root)
    return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--zip", required=True)
    parser.add_argument("--json", required=True)
    parser.add_argument("--out", required=True)
    parser.add_argument("--rename_json", default="False")
    args = parser.parse_args()
    
    zip_path, json_path = Path(args.zip), Path(args.json)
    output_root = Path(args.out)
    rename = args.rename_json.lower() == "true"
    
    final_dir = output_root / zip_path.stem
    temp_dir = output_root / f"temp_{zip_path.stem}"
    
    if temp_dir.exists(): shutil.rmtree(temp_dir)
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            zf.extractall(temp_dir)
        
        data_root = find_data_root(temp_dir)
        if not data_root:
            raise Exception("æœªæ‰¾åˆ°æ•°æ®æ ¹ç›®å½•")
        
        final_dir.mkdir(parents=True, exist_ok=True)
        
        # ç§»åŠ¨ JSON
        target_json = "annotations.json" if rename else json_path.name
        shutil.copy(str(json_path), str(final_dir / target_json))
        
        # ç§»åŠ¨å¿…éœ€æ–‡ä»¶
        keep = ["sample.json", "ins.json", "sensor_config_combined_latest.json",
                "combined_scales", "camera_cam_3M_front", "camera_cam_3M_left",
                "camera_cam_3M_right", "camera_cam_3M_rear", "camera_cam_8M_wa_front",
                "iv_points_front_left", "iv_points_front_mid", "iv_points_front_right",
                "iv_points_rear_left", "iv_points_rear_right"]
        
        for item in keep:
            src = data_root / item
            if src.exists():
                dst = final_dir / item
                if dst.exists():
                    if dst.is_dir(): shutil.rmtree(dst)
                    else: dst.unlink()
                if src.is_dir(): shutil.copytree(str(src), str(dst))
                else: shutil.copy(str(src), str(dst))
        
        print("OK")
    finally:
        if temp_dir.exists(): shutil.rmtree(temp_dir)

if __name__ == "__main__":
    main()
'''
        with self.sftp.file("/tmp/zip_worker.py", "w") as f:
            f.write(worker_code)
        logger.info("å·²éƒ¨ç½²è¿œç¨‹å·¥ä½œè„šæœ¬")
    
    # ==================== æ­¥éª¤ 4: æ£€æŸ¥æ ‡æ³¨è´¨é‡ ====================
    def step4_check_annotations(self):
        """åœ¨æœåŠ¡å™¨ä¸Šç›´æ¥æ£€æŸ¥æ ‡æ³¨è´¨é‡"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 4: æ£€æŸ¥æ ‡æ³¨è´¨é‡ (æœåŠ¡å™¨ç«¯æ‰§è¡Œ)")
        logger.info("=" * 60)
        
        if not self._connect_server():
            return
        
        # è·å–å¤„ç†ç›®å½•ä¸­çš„æ•°æ®
        status, out, err = self._exec_remote(f"ls -d {SERVER_PROCESS_DIR}/*/ 2>/dev/null || true")
        if not out:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¾…æ£€æŸ¥çš„æ•°æ®")
            return
        
        data_dirs = [d.strip().rstrip('/') for d in out.splitlines() if d.strip()]
        logger.info(f"å‘ç° {len(data_dirs)} ä¸ªå¾…æ£€æŸ¥çš„æ•°æ®ç›®å½•")
        
        # éƒ¨ç½²æ£€æŸ¥è„šæœ¬åˆ°æœåŠ¡å™¨
        self._deploy_checker_script()
        
        # åŠ è½½æœ¬åœ°é…ç½®è·å–æ£€æŸ¥è§„åˆ™
        project_root = Path(__file__).parent.parent
        config_path = project_root / CONFIG_PATH
        
        if config_path.exists():
            with open(config_path, 'r') as f:
                base_config = yaml.safe_load(f)
            # ä¸Šä¼ é…ç½®åˆ°æœåŠ¡å™¨
            config_content = yaml.dump(base_config)
            with self.sftp.file("/tmp/check_config.yaml", "w") as f:
                f.write(config_content)
        
        for remote_dir in data_dirs:
            dir_name = Path(remote_dir).name
            
            logger.info(f"æ£€æŸ¥: {dir_name}")
            
            # åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œæ£€æŸ¥è„šæœ¬
            report_path = f"/tmp/report_{dir_name}.txt"
            cmd = f"python3 /tmp/annotation_checker.py --data_dir '{remote_dir}' --config '/tmp/check_config.yaml' --report '{report_path}'"
            
            # æ‰§è¡Œå¹¶å®æ—¶è·å–è¾“å‡º
            stdin, stdout, stderr = self.ssh.exec_command(cmd)
            
            # æ‰“å°è¿œç¨‹è¾“å‡º
            for line in iter(stdout.readline, ""):
                line = line.strip()
                if line:
                    logger.info(f"    [è¿œç¨‹] {line}")
            
            status = stdout.channel.recv_exit_status()
            err = stderr.read().decode().strip()
            
            if status == 0:
                # è¯»å–è¿œç¨‹æŠ¥å‘Š
                try:
                    with self.sftp.file(report_path, 'r') as f:
                        report_content = f.read().decode() if isinstance(f.read(), bytes) else ""
                    
                    # é‡æ–°è¯»å–
                    self.sftp.get(report_path, str(self.local_check_dir / f"report_{dir_name}.txt"))
                    report_content = (self.local_check_dir / f"report_{dir_name}.txt").read_text()
                    
                    issue_count = report_content.count("å¸§:")
                    
                    if issue_count == 0:
                        logger.info(f"    âœ“ æ£€æŸ¥é€šè¿‡ï¼Œæ— é—®é¢˜")
                        self.results['check_passed'].append(dir_name)
                    else:
                        logger.warning(f"    âœ— å‘ç° {issue_count} ä¸ªé—®é¢˜å¸§")
                        logger.warning(f"      æŠ¥å‘Šå·²ä¿å­˜: {self.local_check_dir}/report_{dir_name}.txt")
                        self.results['check_failed'].append(dir_name)
                except Exception as e:
                    # å¦‚æœæ²¡æœ‰æŠ¥å‘Šæ–‡ä»¶ï¼Œè¯´æ˜æ£€æŸ¥é€šè¿‡
                    logger.info(f"    âœ“ æ£€æŸ¥é€šè¿‡")
                    self.results['check_passed'].append(dir_name)
            else:
                logger.error(f"    æ£€æŸ¥å¤±è´¥: {err}")
                self.results['check_failed'].append(dir_name)
        
        logger.info(f"æ£€æŸ¥å®Œæˆ: é€šè¿‡ {len(self.results['check_passed'])}, å¤±è´¥ {len(self.results['check_failed'])}")
    
    def _deploy_checker_script(self):
        """éƒ¨ç½²æ£€æŸ¥è„šæœ¬åˆ°æœåŠ¡å™¨"""
        checker_code = '''
import os
import sys
import json
import math
import argparse
import yaml
import numpy as np
from pathlib import Path

def get_euler_angles(q):
    w, x, y, z = q
    sinr_cosp = 2 * (w * x + y * z)
    cosr_cosp = 1 - 2 * (x * x + y * y)
    roll = math.atan2(sinr_cosp, cosr_cosp)
    sinp = 2 * (w * y - z * x)
    pitch = math.asin(max(-1, min(1, sinp)))
    siny_cosp = 2 * (w * z + x * y)
    cosy_cosp = 1 - 2 * (y * y + z * z)
    yaw = math.atan2(siny_cosp, cosy_cosp)
    return roll, pitch, yaw

def quaternion_to_rotation_matrix(q):
    """å››å…ƒæ•°è½¬æ—‹è½¬çŸ©é˜µ (w, x, y, z) æ ¼å¼"""
    w, x, y, z = q
    return np.array([
        [1 - 2*(y*y + z*z), 2*(x*y - w*z), 2*(x*z + w*y)],
        [2*(x*y + w*z), 1 - 2*(x*x + z*z), 2*(y*z - w*x)],
        [2*(x*z - w*y), 2*(y*z + w*x), 1 - 2*(x*x + y*y)]
    ])

def transform_to_world(pos_ego, ins_entry):
    """å°†è‡ªè½¦åæ ‡ç³»ä¸‹çš„ä½ç½®è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³» (UTM)"""
    ego_utm = np.array([
        ins_entry.get('utm_x', 0),
        ins_entry.get('utm_y', 0),
        ins_entry.get('utm_z', 0)
    ])
    q_ego = [
        ins_entry.get('quaternion_w', 1),
        ins_entry.get('quaternion_x', 0),
        ins_entry.get('quaternion_y', 0),
        ins_entry.get('quaternion_z', 0)
    ]
    R_ego = quaternion_to_rotation_matrix(q_ego)
    pos_world = R_ego @ np.array(pos_ego) + ego_utm
    return pos_world

def check_object(obj, rules, prev_obj=None, next_obj=None, 
                 curr_ins=None, prev_ins=None, next_ins=None):
    issues = []
    size = obj.get('size', [1, 1, 1])
    if len(size) >= 3:
        l, w, h = size[0], size[1], size[2]
    else:
        return issues
    
    obj_class = obj.get('attribute_tokens', {}).get('Class', '').lower()
    
    # å°ºå¯¸æ£€æŸ¥ (ä»…æ£€æŸ¥é•¿å®½ï¼Œä¸æ£€æŸ¥é«˜åº¦)
    if 'vehicle' in obj_class:
        ranges = rules.get('vehicle', {})
        if ranges:
            lr = ranges.get('length_range', [2, 12])
            wr = ranges.get('width_range', [1, 3])
            if not (lr[0] <= l <= lr[1]): issues.append(f"é•¿åº¦å¼‚å¸¸: {l:.2f}")
            if not (wr[0] <= w <= wr[1]): issues.append(f"å®½åº¦å¼‚å¸¸: {w:.2f}")
    
    # å››å…ƒæ•°æ£€æŸ¥
    rotation = obj.get('rotation', [])
    if len(rotation) == 4:
        norm = math.sqrt(sum(x*x for x in rotation))
        if abs(norm - 1.0) > 0.01:
            issues.append(f"å››å…ƒæ•°æœªå½’ä¸€åŒ–: {norm:.4f}")
        
        # å§¿æ€è§’æ£€æŸ¥ (ä»…è½¦è¾†)
        if 'vehicle' in obj_class:
            roll, pitch, yaw = get_euler_angles(rotation)
            if abs(roll) > 0.5:
                issues.append(f"Rollè§’å¼‚å¸¸: {math.degrees(roll):.1f}åº¦")
            if abs(pitch) > 0.5:
                issues.append(f"Pitchè§’å¼‚å¸¸: {math.degrees(pitch):.1f}åº¦")
    
    # æœå‘ä¸è¿åŠ¨æ–¹å‘ä¸€è‡´æ€§æ£€æŸ¥ (è½¦è¾†) - ä½¿ç”¨è‡ªè½¦ä½å§¿è¡¥å¿
    if 'vehicle' in obj_class and (prev_obj or next_obj):
        curr_pos_ego = np.array(obj.get('translation', [0, 0, 0]))
        rotation = obj.get('rotation', [])
        
        # æ˜¯å¦ä½¿ç”¨ä¸–ç•Œåæ ‡ç³»
        use_world = curr_ins is not None
        
        if len(rotation) == 4 and len(curr_pos_ego) >= 2:
            # è½¬æ¢å½“å‰ä½ç½®
            if use_world:
                curr_pos = transform_to_world(curr_pos_ego, curr_ins)
            else:
                curr_pos = curr_pos_ego
            
            # è®¡ç®—è¿åŠ¨å‘é‡
            motion_vec = None
            if prev_obj and next_obj:
                prev_pos_ego = np.array(prev_obj.get('translation', [0, 0, 0]))
                next_pos_ego = np.array(next_obj.get('translation', [0, 0, 0]))
                if use_world and prev_ins and next_ins:
                    prev_pos = transform_to_world(prev_pos_ego, prev_ins)
                    next_pos = transform_to_world(next_pos_ego, next_ins)
                else:
                    prev_pos, next_pos = prev_pos_ego, next_pos_ego
                motion_vec = next_pos[:2] - prev_pos[:2]
            elif next_obj:
                next_pos_ego = np.array(next_obj.get('translation', [0, 0, 0]))
                if use_world and next_ins:
                    next_pos = transform_to_world(next_pos_ego, next_ins)
                else:
                    next_pos = next_pos_ego
                motion_vec = next_pos[:2] - curr_pos[:2]
            elif prev_obj:
                prev_pos_ego = np.array(prev_obj.get('translation', [0, 0, 0]))
                if use_world and prev_ins:
                    prev_pos = transform_to_world(prev_pos_ego, prev_ins)
                else:
                    prev_pos = prev_pos_ego
                motion_vec = curr_pos[:2] - prev_pos[:2]
            
            if motion_vec is not None:
                dist = np.linalg.norm(motion_vec)
                # åªæœ‰ä½ç§»è¶³å¤Ÿå¤§æ—¶æ‰æ£€æŸ¥æœå‘ä¸€è‡´æ€§ (æ’é™¤é™æ­¢ç‰©ä½“)
                if dist > 0.5:
                    motion_yaw = math.atan2(motion_vec[1], motion_vec[0])
                    _, _, obj_yaw_ego = get_euler_angles(rotation)
                    
                    # å¦‚æœä½¿ç”¨ä¸–ç•Œåæ ‡ç³»ï¼Œè½¬æ¢æ ‡æ³¨æœå‘
                    if use_world and curr_ins:
                        ego_yaw = curr_ins.get('azimuth', 0)
                        obj_yaw = ego_yaw + obj_yaw_ego
                    else:
                        obj_yaw = obj_yaw_ego
                    
                    # è®¡ç®—è§’åº¦å·®
                    diff = motion_yaw - obj_yaw
                    while diff > math.pi:
                        diff -= 2 * math.pi
                    while diff < -math.pi:
                        diff += 2 * math.pi
                    diff = abs(diff)
                    
                    # å…è®¸è¯¯å·®60åº¦ï¼Œä¹Ÿå…è®¸å€’è½¦(å·®å€¼æ¥è¿‘180åº¦)
                    is_forward = diff < 1.05  # ~60åº¦
                    is_backward = abs(diff - math.pi) < 1.05
                    
                    if not is_forward and not is_backward:
                        issues.append(f"æœå‘ä¸è¿åŠ¨æ–¹å‘ä¸ä¸€è‡´: å·®å€¼{math.degrees(diff):.1f}åº¦")
    
    return issues

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", required=True)
    parser.add_argument("--config", required=True)
    parser.add_argument("--report", required=True)
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    
    # åŠ è½½é…ç½®
    rules = {}
    if Path(args.config).exists():
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
            rules = config.get('rules', {})
    
    # åŠ è½½ INS æ•°æ® (è‡ªè½¦ä½å§¿)
    ins_data = None
    ins_file = data_dir / 'ins.json'
    if ins_file.exists():
        try:
            with open(ins_file, 'r') as f:
                ins_data = json.load(f)
            print(f"å·²åŠ è½½ INS æ•°æ®: {len(ins_data)} æ¡")
        except Exception as e:
            print(f"åŠ è½½ INS æ•°æ®å¤±è´¥: {e}")
    else:
        print("æœªæ‰¾åˆ° ins.jsonï¼Œå°†ä¸è¿›è¡Œè‡ªè½¦ä½å§¿è¡¥å¿")
    
    # æŸ¥æ‰¾æ ‡æ³¨æ–‡ä»¶
    annotation_file = None
    for name in ['annotations.json', 'annotation.json']:
        p = data_dir / name
        if p.exists():
            annotation_file = p
            break
    
    if not annotation_file:
        for f in data_dir.glob('*.json'):
            if f.name not in ['sample.json', 'ins.json', 'sensor_config_combined_latest.json']:
                annotation_file = f
                break
    
    if not annotation_file:
        print("ERROR: æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶")
        sys.exit(1)
    
    # åŠ è½½æ ‡æ³¨
    print(f"åŠ è½½æ ‡æ³¨æ–‡ä»¶: {annotation_file.name}")
    with open(annotation_file, 'r') as f:
        data = json.load(f)
    
    # æ£€æŸ¥
    issues_by_frame = {}
    total_frames = 0
    total_objects = 0
    issue_objects = 0
    
    frames_to_check = []
    
    if isinstance(data, dict):
        if 'frames' in data:
            for frame in data['frames']:
                frame_id = frame.get('frame_id', frame.get('id', 'unknown'))
                objects = frame.get('objects', [])
                frames_to_check.append((str(frame_id), objects))
        else:
            for frame_id, objects in data.items():
                if isinstance(objects, list):
                    frames_to_check.append((str(frame_id), objects))
    
    try:
        frames_to_check.sort(key=lambda x: int(x[0]))
    except:
        frames_to_check.sort(key=lambda x: x[0])
    
    total_frames = len(frames_to_check)
    print(f"å¼€å§‹æ£€æŸ¥ {total_frames} å¸§...")
    
    # æ„å»º INS ç´¢å¼• (æŒ‰å¸§ç´¢å¼•)
    frame_to_ins = {}
    if ins_data:
        for i in range(min(len(frames_to_check), len(ins_data))):
            frame_to_ins[i] = ins_data[i]
    
    # æ„å»ºå®ä¾‹è½¨è¿¹
    tracks = {}
    for i, (frame_id, objects) in enumerate(frames_to_check):
        for obj in objects:
            inst_id = obj.get('instance_token')
            if inst_id:
                if inst_id not in tracks:
                    tracks[inst_id] = []
                tracks[inst_id].append((i, obj))
    
    for inst_id in tracks:
        tracks[inst_id].sort(key=lambda x: x[0])
    
    for i, (frame_id, objects) in enumerate(frames_to_check):
        if (i + 1) % 20 == 0 or i == total_frames - 1:
            print(f"  è¿›åº¦: {i+1}/{total_frames} ({(i+1)*100//total_frames}%)")
        
        total_objects += len(objects)
        curr_ins = frame_to_ins.get(i)
        
        frame_issues = []
        for obj in objects:
            prev_obj, next_obj = None, None
            prev_ins, next_ins = None, None
            inst_id = obj.get('instance_token')
            if inst_id and inst_id in tracks:
                track = tracks[inst_id]
                for idx, (frame_idx, track_obj) in enumerate(track):
                    if frame_idx == i:
                        if idx > 0:
                            prev_obj = track[idx - 1][1]
                            prev_ins = frame_to_ins.get(track[idx - 1][0])
                        if idx < len(track) - 1:
                            next_obj = track[idx + 1][1]
                            next_ins = frame_to_ins.get(track[idx + 1][0])
                        break
            
            obj_issues = check_object(obj, rules, prev_obj, next_obj,
                                       curr_ins, prev_ins, next_ins)
            if obj_issues:
                issue_objects += 1
                frame_issues.append({
                    'token': obj.get('token', 'unknown'),
                    'class': obj.get('attribute_tokens', {}).get('Class', 'unknown'),
                    'issues': obj_issues
                })
        if frame_issues:
            issues_by_frame[str(frame_id)] = frame_issues
    
    issue_frames = len(issues_by_frame)
    print(f"\\næ£€æŸ¥å®Œæˆ!")
    print(f"  æ€»å¸§æ•°: {total_frames}")
    print(f"  æ€»å¯¹è±¡æ•°: {total_objects}")
    print(f"  é—®é¢˜å¸§æ•°: {issue_frames}")
    print(f"  é—®é¢˜å¯¹è±¡æ•°: {issue_objects}")
    
    with open(args.report, 'w') as f:
        f.write(f"æ£€æŸ¥æŠ¥å‘Š - {data_dir.name}\\n")
        f.write("=" * 50 + "\\n\\n")
        f.write(f"ç»Ÿè®¡æ±‡æ€»:\\n")
        f.write(f"  æ€»å¸§æ•°: {total_frames}\\n")
        f.write(f"  æ€»å¯¹è±¡æ•°: {total_objects}\\n")
        f.write(f"  é—®é¢˜å¸§æ•°: {issue_frames}\\n")
        f.write(f"  é—®é¢˜å¯¹è±¡æ•°: {issue_objects}\\n")
        f.write(f"  é€šè¿‡ç‡: {(total_frames - issue_frames) * 100 / max(total_frames, 1):.1f}%\\n")
        if ins_data:
            f.write(f"  è‡ªè½¦ä½å§¿è¡¥å¿: å·²å¯ç”¨ ({len(ins_data)} æ¡INSæ•°æ®)\\n")
        else:
            f.write(f"  è‡ªè½¦ä½å§¿è¡¥å¿: æœªå¯ç”¨\\n")
        f.write("\\n" + "=" * 50 + "\\n\\n")
        
        if not issues_by_frame:
            f.write("æ­å–œ! æ‰€æœ‰å¸§æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°é—®é¢˜ã€‚\\n")
        else:
            f.write("é—®é¢˜è¯¦æƒ…:\\n\\n")
            for frame_id, issues in sorted(issues_by_frame.items(), key=lambda x: int(x[0]) if x[0].isdigit() else x[0]):
                f.write(f"å¸§: {frame_id}\\n")
                for item in issues:
                    f.write(f"  å¯¹è±¡: {item['token']} (ç±»åˆ«: {item['class']})\\n")
                    for issue in item['issues']:
                        f.write(f"    - {issue}\\n")
                f.write("\\n")
    
    if issue_frames == 0:
        print("RESULT: PASS")
    else:
        print(f"RESULT: FAIL ({issue_frames} frames with issues)")

if __name__ == "__main__":
    main()
'''
        with self.sftp.file("/tmp/annotation_checker.py", "w") as f:
            f.write(checker_code)
        logger.info("å·²éƒ¨ç½²æ£€æŸ¥è„šæœ¬åˆ°æœåŠ¡å™¨")
    
    def _download_dir(self, remote_dir: str, local_dir: Path):
        """é€’å½’ä¸‹è½½è¿œç¨‹ç›®å½•"""
        for item in self.sftp.listdir_attr(remote_dir):
            remote_path = f"{remote_dir}/{item.filename}"
            local_path = local_dir / item.filename
            
            if item.st_mode & 0o40000:  # æ˜¯ç›®å½•
                local_path.mkdir(exist_ok=True)
                self._download_dir(remote_path, local_path)
            else:
                self.sftp.get(remote_path, str(local_path))
    
    # ==================== æ­¥éª¤ 5: ç§»åŠ¨é€šè¿‡çš„æ•°æ®åˆ°æœ€ç»ˆç›®å½• ====================
    def step5_move_to_final(self):
        """å°†æ£€æŸ¥é€šè¿‡çš„æ•°æ®ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•"""
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 5: ç§»åŠ¨é€šè¿‡çš„æ•°æ®åˆ°æœ€ç»ˆç›®å½•")
        logger.info("=" * 60)
        
        if not self.results['check_passed']:
            logger.warning("æ²¡æœ‰æ£€æŸ¥é€šè¿‡çš„æ•°æ®éœ€è¦ç§»åŠ¨")
            return
        
        if not self._connect_server():
            return
        
        # ç¡®ä¿æœ€ç»ˆç›®å½•å­˜åœ¨
        self._exec_remote(f"mkdir -p {SERVER_FINAL_DIR}")
        
        for dir_name in self.results['check_passed']:
            src = f"{SERVER_PROCESS_DIR}/{dir_name}"
            dst = f"{SERVER_FINAL_DIR}/{dir_name}"
            
            logger.info(f"ç§»åŠ¨: {dir_name}")
            
            # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨
            status, _, _ = self._exec_remote(f"test -d '{src}' && echo ok")
            if status != 0:
                logger.warning(f"    æºç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            # å®‰å…¨ç§»åŠ¨ï¼šå¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½è€Œä¸æ˜¯ç›´æ¥åˆ é™¤
            status, out, _ = self._exec_remote(f"test -d '{dst}' && echo exists")
            if out.strip() == 'exists':
                backup_dst = f"{dst}.bak.{datetime.now().strftime('%Y%m%d%H%M%S')}"
                self._exec_remote(f"mv '{dst}' '{backup_dst}'")
                logger.info(f"    å·²å¤‡ä»½æ—§æ•°æ®åˆ°: {backup_dst}")
            
            # ç§»åŠ¨ç›®å½•
            status, _, err = self._exec_remote(f"mv '{src}' '{dst}'")
            if status == 0:
                logger.info(f"    âœ“ å·²ç§»åŠ¨åˆ° {dst}")
                self.results['moved_to_final'].append(dir_name)
            else:
                logger.error(f"    ç§»åŠ¨å¤±è´¥: {err}")
                self._log_error(dir_name, "ç§»åŠ¨", f"ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•å¤±è´¥: {err}")
        
        logger.info(f"ç§»åŠ¨å®Œæˆ: {len(self.results['moved_to_final'])} ä¸ªç›®å½•")
        
        logger.info(f"ç§»åŠ¨å®Œæˆ: {len(self.results['moved_to_final'])} ä¸ªç›®å½•")
    
    # ==================== æµå¼å¤„ç†ï¼šä¸‹è½½ä¸€ä¸ªå¤„ç†ä¸€ä¸ª ====================
    def run_streaming(self):
        """
        æµå¼å¤„ç†æ¨¡å¼ï¼šä¸‹è½½ä¸€ä¸ªæ–‡ä»¶åç«‹å³è¿›è¡Œå®Œæ•´å¤„ç†æµç¨‹
        æ¯ä¸ªæ–‡ä»¶ï¼šä¸‹è½½ -> ä¸Šä¼  -> è§£å‹å¤„ç† -> æ£€æŸ¥ -> ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•
        """
        import requests
        
        logger.info("=" * 60)
        logger.info("æ ‡æ³¨æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æµæ°´çº¿ (æµå¼å¤„ç†æ¨¡å¼)")
        logger.info(f"JSON ç›®å½•: {self.json_dir}")
        logger.info("å¤„ç†æ–¹å¼: ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶å°±ç«‹å³å¤„ç†ï¼Œæ— éœ€ç­‰å¾…å…¨éƒ¨ä¸‹è½½")
        logger.info("=" * 60)
        
        json_files = list(self.json_dir.glob("*.json"))
        if not json_files:
            logger.warning(f"æœªåœ¨ {self.json_dir} æ‰¾åˆ° JSON æ–‡ä»¶")
            return
        
        logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶éœ€è¦å¤„ç†")
        
        try:
            # è¿æ¥æœåŠ¡å™¨
            if not self._connect_server():
                logger.error("æ— æ³•è¿æ¥æœåŠ¡å™¨ï¼Œé€€å‡º")
                return
            
            # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
            self._exec_remote(f"mkdir -p {SERVER_ZIP_DIR}")
            self._exec_remote(f"mkdir -p {SERVER_PROCESS_DIR}")
            
            # éƒ¨ç½²è¿œç¨‹è„šæœ¬
            self._deploy_worker_script()
            self._deploy_checker_script()
            
            # ä¸Šä¼ æ£€æŸ¥é…ç½®
            project_root = Path(__file__).parent.parent
            config_path = project_root / CONFIG_PATH
            if config_path.exists():
                with open(config_path, 'r') as f:
                    base_config = yaml.safe_load(f)
                config_content = yaml.dump(base_config)
                with self.sftp.file("/tmp/check_config.yaml", "w") as f:
                    f.write(config_content)
            
            # è·å–æœåŠ¡å™¨ä¸Šå·²æœ‰çš„ ZIP æ–‡ä»¶ï¼ˆå«å·²å¤„ç†çš„ï¼‰
            server_zip_originals = set()
            status, out, err = self._exec_remote(f"ls {SERVER_ZIP_DIR}/*.zip 2>/dev/null || true")
            if out:
                for line in out.splitlines():
                    name = Path(line.strip()).name
                    if name.startswith("processed_"):
                        server_zip_originals.add(name[len("processed_"):])
                    else:
                        server_zip_originals.add(name)
            logger.info(f"æœåŠ¡å™¨ä¸Šå·²æœ‰ {len(server_zip_originals)} ä¸ª ZIP æ–‡ä»¶")
            
            # è·å–å·²å¤„ç†å®Œæˆçš„ç›®å½•ï¼ˆä»…æ£€æŸ¥æœ€ç»ˆç›®å½•ï¼‰
            processed_dirs = set()
            status, out, err = self._exec_remote(f"ls -d {SERVER_FINAL_DIR}/*/ 2>/dev/null || true")
            if out:
                for line in out.splitlines():
                    dir_name = Path(line.strip().rstrip('/')).name
                    processed_dirs.add(dir_name)
            logger.info(f"å·²å¤„ç†å®Œæˆçš„æ•°æ®ï¼ˆæœ€ç»ˆç›®å½•ï¼‰: {len(processed_dirs)} ä¸ª")
            
            # è‡ªåŠ¨è·å– Token
            auth_token = self._get_dataweave_token()
            headers = {
                "User-Agent": "Mozilla/5.0",
                "Content-Type": "application/json",
                "Authorization": auth_token,
            }
            
            # é€ä¸ªå¤„ç†æ¯ä¸ª JSON æ–‡ä»¶
            for i, json_file in enumerate(json_files):
                stem = json_file.stem
                zip_name = f"{stem}.zip"
                
                logger.info("")
                logger.info(f"{'='*60}")
                logger.info(f"[{i+1}/{len(json_files)}] å¤„ç†æ–‡ä»¶: {stem}")
                logger.info(f"{'='*60}")
                
                # æ£€æŸ¥æ˜¯å¦å·²å®Œå…¨å¤„ç†è¿‡
                if stem in processed_dirs:
                    logger.info(f"  â†’ å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
                    self.results['check_passed'].append(stem)
                    continue
                
                # ===== æ­¥éª¤ 1: ä¸‹è½½æˆ–æ£€æŸ¥ =====
                local_zip = self.local_zip_dir / zip_name
                remote_zip = f"{SERVER_ZIP_DIR}/{zip_name}"
                need_download = True
                
                # æ£€æŸ¥æœåŠ¡å™¨ä¸Šæ˜¯å¦å·²å­˜åœ¨
                if zip_name in server_zip_originals:
                    logger.info(f"  [ä¸‹è½½] æœåŠ¡å™¨ä¸Šå·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
                    self.results['skipped_server_exists'].append(stem)
                    need_download = False
                elif self._is_valid_zip(local_zip):
                    logger.info(f"  [ä¸‹è½½] æœ¬åœ°å·²å­˜åœ¨ä¸”å®Œæ•´: {local_zip}")
                    need_download = False
                
                if need_download:
                    # ä» DataWeave ä¸‹è½½
                    logger.info(f"  [ä¸‹è½½] æ­£åœ¨ä» DataWeave ä¸‹è½½...")
                    downloaded = self._download_single_zip(stem, zip_name, local_zip, headers)
                    if not downloaded:
                        logger.error(f"  [ä¸‹è½½] ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                        continue
                    self.results['downloaded'].append(stem)
                    logger.info(f"  [ä¸‹è½½] âœ“ ä¸‹è½½å®Œæˆ")
                
                # ===== æ­¥éª¤ 2: ä¸Šä¼ åˆ°æœåŠ¡å™¨ =====
                if zip_name not in server_zip_originals and local_zip.exists():
                    logger.info(f"  [ä¸Šä¼ ] æ­£åœ¨ä¸Šä¼ åˆ°æœåŠ¡å™¨...")
                    try:
                        self.sftp.put(str(local_zip), remote_zip)
                        self.results['uploaded'].append(stem)
                        logger.info(f"  [ä¸Šä¼ ] âœ“ ä¸Šä¼ å®Œæˆ")
                    except Exception as e:
                        logger.error(f"  [ä¸Šä¼ ] ä¸Šä¼ å¤±è´¥: {e}")
                        continue
                
                # ===== æ­¥éª¤ 3: æœåŠ¡å™¨ç«¯å¤„ç† =====
                logger.info(f"  [å¤„ç†] æ­£åœ¨æœåŠ¡å™¨ç«¯è§£å‹å¤„ç†...")
                
                # ä¸Šä¼  JSON æ–‡ä»¶
                remote_json_temp = f"/tmp/{json_file.name}"
                try:
                    self.sftp.put(str(json_file), remote_json_temp)
                except Exception as e:
                    logger.error(f"  [å¤„ç†] ä¸Šä¼  JSON å¤±è´¥: {e}")
                    continue
                
                # æ‰§è¡Œå¤„ç†è„šæœ¬
                cmd = f"python3 /tmp/zip_worker.py --zip '{remote_zip}' --json '{remote_json_temp}' --out '{SERVER_PROCESS_DIR}' --rename_json '{RENAME_JSON}'"
                status, out, err = self._exec_remote(cmd)
                
                if status != 0:
                    logger.error(f"  [å¤„ç†] å¤„ç†å¤±è´¥: {err}")
                    continue
                
                # å¤„ç†åŸå§‹ ZIP
                if ZIP_AFTER_PROCESS == "rename":
                    new_name = f"{SERVER_ZIP_DIR}/processed_{zip_name}"
                    self._exec_remote(f"mv '{remote_zip}' '{new_name}'")
                elif ZIP_AFTER_PROCESS == "delete":
                    self._exec_remote(f"rm '{remote_zip}'")
                
                self.results['processed'].append(stem)
                logger.info(f"  [å¤„ç†] âœ“ å¤„ç†å®Œæˆ")
                
                # ===== æ­¥éª¤ 4: æ£€æŸ¥æ ‡æ³¨è´¨é‡ =====
                logger.info(f"  [æ£€æŸ¥] æ­£åœ¨æ£€æŸ¥æ ‡æ³¨è´¨é‡...")
                
                remote_data_dir = f"{SERVER_PROCESS_DIR}/{stem}"
                report_path = f"/tmp/report_{stem}.txt"
                cmd = f"python3 /tmp/annotation_checker.py --data_dir '{remote_data_dir}' --config '/tmp/check_config.yaml' --report '{report_path}'"
                
                status, out, err = self._exec_remote(cmd)
                
                check_passed = True
                local_report = self.local_check_dir / f"report_{stem}.txt"
                if status == 0:
                    try:
                        # ä¸‹è½½æŠ¥å‘Šæ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
                        self.sftp.get(report_path, str(local_report))
                        report_content = local_report.read_text()
                        issue_count = report_content.count("å¸§:")
                        
                        if issue_count > 0:
                            check_passed = False
                            logger.warning(f"  [æ£€æŸ¥] âœ— å‘ç° {issue_count} ä¸ªé—®é¢˜å¸§")
                            logger.warning(f"         æŠ¥å‘Š: {local_report}")
                        else:
                            # æ£€æŸ¥é€šè¿‡ï¼Œåˆ é™¤æœ¬åœ°æŠ¥å‘Š
                            if local_report.exists():
                                local_report.unlink()
                    except:
                        pass  # æ²¡æœ‰æŠ¥å‘Šæ–‡ä»¶è¯´æ˜é€šè¿‡
                else:
                    check_passed = False
                    logger.error(f"  [æ£€æŸ¥] æ£€æŸ¥æ‰§è¡Œå¤±è´¥: {err}")
                
                if check_passed:
                    logger.info(f"  [æ£€æŸ¥] âœ“ æ£€æŸ¥é€šè¿‡")
                    self.results['check_passed'].append(stem)
                    
                    # ===== æ­¥éª¤ 5: ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½• =====
                    logger.info(f"  [ç§»åŠ¨] æ­£åœ¨ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•...")
                    src = f"{SERVER_PROCESS_DIR}/{stem}"
                    dst = f"{SERVER_FINAL_DIR}/{stem}"
                    
                    # å®‰å…¨ç§»åŠ¨ï¼šå…ˆæ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™å¤‡ä»½è€Œä¸æ˜¯ç›´æ¥åˆ é™¤
                    status, out, _ = self._exec_remote(f"test -d '{dst}' && echo exists")
                    if out.strip() == 'exists':
                        backup_dst = f"{dst}.bak.{datetime.now().strftime('%Y%m%d%H%M%S')}"
                        self._exec_remote(f"mv '{dst}' '{backup_dst}'")
                        logger.info(f"  [ç§»åŠ¨] å·²å¤‡ä»½æ—§æ•°æ®åˆ°: {backup_dst}")
                    
                    status, _, err = self._exec_remote(f"mv '{src}' '{dst}'")
                    
                    if status == 0:
                        logger.info(f"  [ç§»åŠ¨] âœ“ å·²ç§»åŠ¨åˆ° {dst}")
                        self.results['moved_to_final'].append(stem)
                        # æµæ°´çº¿æˆåŠŸå®Œæˆï¼Œåˆ é™¤æœ¬åœ° ZIP æ–‡ä»¶
                        if local_zip.exists():
                            local_zip.unlink()
                            logger.info(f"  [æ¸…ç†] å·²åˆ é™¤æœ¬åœ° ZIP: {local_zip.name}")
                    else:
                        logger.error(f"  [ç§»åŠ¨] ç§»åŠ¨å¤±è´¥: {err}")
                        self._log_error(stem, "ç§»åŠ¨", f"ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•å¤±è´¥: {err}")
                else:
                    self.results['check_failed'].append(stem)
                    self._log_error(stem, "æ£€æŸ¥", f"æ£€æŸ¥æœªé€šè¿‡")
                
                logger.info(f"  â†’ æ–‡ä»¶å¤„ç†å®Œæˆ")
        
        finally:
            self._close_server()
        
        # è¾“å‡ºæ±‡æ€»
        self._print_summary()
    
    # ==================== å¤šçº¿ç¨‹å¤„ç†æ¨¡å¼ ====================
    def run_parallel(self, num_workers: int = None):
        """
        å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ¨¡å¼ï¼šå¤šä¸ªçº¿ç¨‹åŒæ—¶ä¸‹è½½å’Œå¤„ç†æ–‡ä»¶
        æ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹å¤„ç†ä¸€ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹
        """
        import requests
        
        if num_workers is None:
            num_workers = MAX_WORKERS
        
        print()
        print("â•”" + "â•" * 50 + "â•—")
        print("â•‘  ğŸ“¦ æ ‡æ³¨æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æµæ°´çº¿ (å¹¶è¡Œæ¨¡å¼)".ljust(51) + "â•‘")
        print("â•š" + "â•" * 50 + "â•")
        print(f"  ğŸ“ JSONç›®å½•: {self.json_dir}")
        
        json_files = list(self.json_dir.glob("*.json"))
        if not json_files:
            print(f"  âš  æœªæ‰¾åˆ° JSON æ–‡ä»¶")
            return
        
        print(f"  ğŸ“‹ å…± {len(json_files)} ä¸ªæ–‡ä»¶")
        
        # å…ˆç”¨ä¸»è¿æ¥åˆå§‹åŒ–
        if not self._connect_server():
            print("  âœ— æ— æ³•è¿æ¥æœåŠ¡å™¨")
            return
        
        print(f"  ğŸ”— å·²è¿æ¥æœåŠ¡å™¨: {SERVER_IP}")
        
        try:
            # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
            self._exec_remote(f"mkdir -p {SERVER_ZIP_DIR}")
            self._exec_remote(f"mkdir -p {SERVER_PROCESS_DIR}")
            
            # éƒ¨ç½²è¿œç¨‹è„šæœ¬
            self._deploy_worker_script()
            self._deploy_checker_script()
            
            # ä¸Šä¼ æ£€æŸ¥é…ç½®
            project_root = Path(__file__).parent.parent
            config_path = project_root / CONFIG_PATH
            if config_path.exists():
                with open(config_path, 'r') as f:
                    base_config = yaml.safe_load(f)
                config_content = yaml.dump(base_config)
                with self.sftp.file("/tmp/check_config.yaml", "w") as f:
                    f.write(config_content)
            
            # è·å–æœåŠ¡å™¨ä¸Šå·²æœ‰çš„ ZIP æ–‡ä»¶ï¼ˆå«å·²å¤„ç†çš„ï¼‰
            server_zip_originals = set()
            status, out, err = self._exec_remote(f"ls {SERVER_ZIP_DIR}/*.zip 2>/dev/null || true")
            if out:
                for line in out.splitlines():
                    name = Path(line.strip()).name
                    if name.startswith("processed_"):
                        server_zip_originals.add(name[len("processed_"):])
                    else:
                        server_zip_originals.add(name)
            
            # è·å–å·²å¤„ç†å®Œæˆçš„ç›®å½•ï¼ˆä»…æ£€æŸ¥æœ€ç»ˆç›®å½•ï¼‰
            processed_dirs = set()
            status, out, err = self._exec_remote(f"ls -d {SERVER_FINAL_DIR}/*/ 2>/dev/null || true")
            if out:
                for line in out.splitlines():
                    dir_name = Path(line.strip().rstrip('/')).name
                    processed_dirs.add(dir_name)
            
            print(f"  ğŸ“Š æœåŠ¡å™¨çŠ¶æ€: {len(server_zip_originals)} ZIPs / {len(processed_dirs)} å·²å®Œæˆ")
            
            # è‡ªåŠ¨è·å– Token
            auth_token = self._get_dataweave_token()
            
            # å…³é—­ä¸»è¿æ¥ï¼Œè®©æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„è¿æ¥
            self._close_server()
            
            # è¿‡æ»¤éœ€è¦å¤„ç†çš„æ–‡ä»¶
            files_to_process = []
            skipped_count = 0
            for i, json_file in enumerate(json_files):
                stem = json_file.stem
                if stem in processed_dirs:
                    skipped_count += 1
                    with results_lock:
                        self.results['check_passed'].append(stem)
                else:
                    files_to_process.append((i, json_file, stem))
            
            if skipped_count > 0:
                print(f"  â­ è·³è¿‡å·²å®Œæˆ: {skipped_count} ä¸ª")
            
            if not files_to_process:
                print("  âœ“ æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
                return
            
            print(f"  ğŸ“¦ å¾…å¤„ç†: {len(files_to_process)} ä¸ªæ–‡ä»¶")
            print(f"  ğŸ§µ çº¿ç¨‹æ•°: {num_workers}")
            print()
            
            # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
            progress = ProgressTracker(len(files_to_process), "å¹¶è¡Œå¤„ç†")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
            with ThreadPoolExecutor(max_workers=num_workers, thread_name_prefix='Worker') as executor:
                futures = {}
                for i, json_file, stem in files_to_process:
                    future = executor.submit(
                        self._process_single_file_threaded,
                        i + 1,
                        len(json_files),
                        json_file,
                        stem,
                        server_zip_originals,
                        auth_token
                    )
                    futures[future] = stem
                
                # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ›´æ–°è¿›åº¦
                for future in as_completed(futures):
                    stem = futures[future]
                    try:
                        result = future.result()
                        progress.update(success=result, name=stem)
                    except Exception as e:
                        progress.update(success=False, name=f"{stem} (å¼‚å¸¸)")
            
            # æ˜¾ç¤ºæ±‡æ€»
            progress.summary()
        
        finally:
            self._close_server()
        
        # è¾“å‡ºè¯¦ç»†æ±‡æ€»
        self._print_summary()
    
    def _log_error(self, stem: str, step: str, error_msg: str):
        """è®°å½•é”™è¯¯ä¿¡æ¯ç”¨äºè¿½æº¯"""
        with self.errors_lock:
            if stem not in self.errors:
                self.errors[stem] = []
            self.errors[stem].append((step, error_msg))
    
    def _process_single_file_threaded(self, idx: int, total: int, json_file: Path, stem: str, 
                                       server_zip_originals: set, auth_token: str) -> bool:
        """
        çº¿ç¨‹å®‰å…¨çš„å•æ–‡ä»¶å¤„ç†å‡½æ•° (é™é»˜æ¨¡å¼ï¼Œä¸è¾“å‡ºæ—¥å¿—)
        å¤±è´¥æ—¶è®°å½•é”™è¯¯åˆ° self.errors ä»¥ä¾¿è¿½æº¯
        æ”¯æŒ SSH æ–­çº¿é‡è¿å’Œ Token è‡ªåŠ¨åˆ·æ–°
        """
        import paramiko
        import requests
        import time
        
        zip_name = f"{stem}.zip"
        local_zip = self.local_zip_dir / zip_name
        remote_zip = f"{SERVER_ZIP_DIR}/{zip_name}"
        
        max_ssh_retries = 3
        
        def connect_ssh():
            """åˆ›å»º SSH è¿æ¥ï¼Œæ”¯æŒé‡è¯•"""
            for attempt in range(max_ssh_retries):
                try:
                    _ssh = paramiko.SSHClient()
                    _ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
                    _ssh.connect(SERVER_IP, username=SERVER_USER, timeout=30)
                    _sftp = _ssh.open_sftp()
                    return _ssh, _sftp
                except Exception as e:
                    if attempt < max_ssh_retries - 1:
                        time.sleep(2 * (attempt + 1))
                        continue
                    raise e
            return None, None
        
        ssh = None
        sftp = None
        
        try:
            ssh, sftp = connect_ssh()
            if not ssh:
                self._log_error(stem, "è¿æ¥", "æ— æ³•å»ºç«‹ SSH è¿æ¥")
                return False
            
            # ===== æ­¥éª¤ 1: ä¸‹è½½ =====
            need_download = True
            if zip_name in server_zip_originals:
                with results_lock:
                    self.results['skipped_server_exists'].append(stem)
                need_download = False
            elif self._is_valid_zip(local_zip):
                need_download = False
            
            if need_download:
                # è·å–æœ€æ–°çš„ Token (ä¼šè‡ªåŠ¨åˆ·æ–°è¿‡æœŸçš„ Token)
                current_token = self._get_dataweave_token()
                headers = {
                    "User-Agent": "Mozilla/5.0",
                    "Content-Type": "application/json",
                    "Authorization": current_token,
                }
                downloaded = self._download_single_zip(stem, zip_name, local_zip, headers)
                if not downloaded:
                    self._log_error(stem, "ä¸‹è½½", "ä¸‹è½½å¤±è´¥ï¼Œæ–‡ä»¶åœ¨DataWeaveä¸­ä¸å­˜åœ¨æˆ–ç½‘ç»œé—®é¢˜")
                    return False
                with results_lock:
                    self.results['downloaded'].append(stem)
            
            # ===== æ­¥éª¤ 2: ä¸Šä¼  =====
            if zip_name not in server_zip_originals and local_zip.exists():
                upload_ok = False
                for upload_attempt in range(3):
                    try:
                        sftp.put(str(local_zip), remote_zip)
                        upload_ok = True
                        break
                    except Exception as e:
                        if upload_attempt < 2:
                            # å°è¯•é‡è¿
                            try:
                                if sftp: sftp.close()
                                if ssh: ssh.close()
                            except: pass
                            import time
                            time.sleep(2)
                            ssh, sftp = connect_ssh()
                            if not ssh:
                                break
                        else:
                            self._log_error(stem, "ä¸Šä¼ ", f"ä¸Šä¼ åˆ°æœåŠ¡å™¨å¤±è´¥: {e}")
                            return False
                if not upload_ok:
                    self._log_error(stem, "ä¸Šä¼ ", "ä¸Šä¼ å¤±è´¥ï¼Œæ— æ³•å»ºç«‹è¿æ¥")
                    return False
                with results_lock:
                    self.results['uploaded'].append(stem)
            
            # ===== æ­¥éª¤ 3: æœåŠ¡å™¨å¤„ç† =====
            remote_json_temp = f"/tmp/{json_file.name}"
            try:
                sftp.put(str(json_file), remote_json_temp)
            except Exception as e:
                self._log_error(stem, "ä¸Šä¼ JSON", f"ä¸Šä¼ JSONæ–‡ä»¶å¤±è´¥: {e}")
                return False
            
            cmd = f"python3 /tmp/zip_worker.py --zip '{remote_zip}' --json '{remote_json_temp}' --out '{SERVER_PROCESS_DIR}' --rename_json '{RENAME_JSON}'"
            status, _, err_output = self._exec_remote_thread(ssh, cmd, timeout=300)
            
            if status != 0:
                self._log_error(stem, "æœåŠ¡å™¨å¤„ç†", f"å¤„ç†è„šæœ¬è¿”å›é”™è¯¯ç  {status}: {err_output[:200]}")
                return False
            
            # å¤„ç†åŸå§‹ ZIP
            if ZIP_AFTER_PROCESS == "rename":
                new_name = f"{SERVER_ZIP_DIR}/processed_{zip_name}"
                ssh.exec_command(f"mv '{remote_zip}' '{new_name}'")
            elif ZIP_AFTER_PROCESS == "delete":
                ssh.exec_command(f"rm '{remote_zip}'")
            
            with results_lock:
                self.results['processed'].append(stem)
            
            # ===== æ­¥éª¤ 4: æ£€æŸ¥ =====
            remote_data_dir = f"{SERVER_PROCESS_DIR}/{stem}"
            report_path = f"/tmp/report_{stem}.txt"
            cmd = f"python3 /tmp/annotation_checker.py --data_dir '{remote_data_dir}' --config '/tmp/check_config.yaml' --report '{report_path}'"
            
            status, _, check_err = self._exec_remote_thread(ssh, cmd, timeout=120)
            
            check_passed = True
            local_report = self.local_check_dir / f"report_{stem}.txt"
            
            if status == 0:
                try:
                    sftp.get(report_path, str(local_report))
                    report_content = local_report.read_text()
                    issue_count = report_content.count("å¸§:")
                    if issue_count > 0:
                        check_passed = False
                        self._log_error(stem, "æ£€æŸ¥", f"å‘ç° {issue_count} ä¸ªé—®é¢˜å¸§ï¼Œè¯¦è§æŠ¥å‘Š: {local_report}")
                    else:
                        # æ£€æŸ¥é€šè¿‡ï¼Œåˆ é™¤æœ¬åœ°æŠ¥å‘Š
                        if local_report.exists():
                            local_report.unlink()
                except Exception as e:
                    self._log_error(stem, "æ£€æŸ¥", f"è·å–æŠ¥å‘Šå¤±è´¥: {e}")
            else:
                check_passed = False
                self._log_error(stem, "æ£€æŸ¥", f"æ£€æŸ¥è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ç  {status}: {check_err[:200]}")
            
            if check_passed:
                with results_lock:
                    self.results['check_passed'].append(stem)
                
                # ===== æ­¥éª¤ 5: ç§»åŠ¨ =====
                src = f"{SERVER_PROCESS_DIR}/{stem}"
                dst = f"{SERVER_FINAL_DIR}/{stem}"
                
                # å®‰å…¨ç§»åŠ¨ï¼šå…ˆæ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨ï¼Œå­˜åœ¨åˆ™å¤‡ä»½è€Œä¸æ˜¯ç›´æ¥åˆ é™¤
                status, out, _ = self._exec_remote_thread(ssh, f"test -d '{dst}' && echo exists")
                if out.strip() == 'exists':
                    # ç›®æ ‡å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½æ—§æ•°æ®
                    backup_dst = f"{dst}.bak.{datetime.now().strftime('%Y%m%d%H%M%S')}"
                    ssh.exec_command(f"mv '{dst}' '{backup_dst}'")
                
                status, _, move_err = self._exec_remote_thread(ssh, f"mv '{src}' '{dst}'")
                
                if status == 0:
                    with results_lock:
                        self.results['moved_to_final'].append(stem)
                    # æµæ°´çº¿æˆåŠŸå®Œæˆï¼Œåˆ é™¤æœ¬åœ° ZIP æ–‡ä»¶
                    if local_zip.exists():
                        local_zip.unlink()
                else:
                    self._log_error(stem, "ç§»åŠ¨", f"ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•å¤±è´¥: {move_err}")
            else:
                with results_lock:
                    self.results['check_failed'].append(stem)
            
            return check_passed
            
        except Exception as e:
            self._log_error(stem, "å¼‚å¸¸", f"{type(e).__name__}: {str(e)}")
            return False
        
        finally:
            try:
                if sftp:
                    sftp.close()
                if ssh:
                    ssh.close()
            except:
                pass

    def _download_single_zip(self, stem: str, zip_name: str, target_file: Path, headers: dict, 
                              retry_token: bool = True) -> bool:
        """ä¸‹è½½å•ä¸ª ZIP æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…ä¸‹è½½ä¸­æ–­å¯¼è‡´çš„ä¸å®Œæ•´æ–‡ä»¶
        
        æ”¯æŒ Token è¿‡æœŸè‡ªåŠ¨åˆ·æ–°å’Œä¸‹è½½é‡è¯•
        """
        import requests
        import time
        
        # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        temp_file = target_file.with_suffix('.zip.tmp')
        
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # åœ¨å¤šä¸ªè·¯å¾„æ¨¡æ¿ä¸­æŸ¥æ‰¾æ–‡ä»¶
                real_url = None
                found_path = None
                token_expired = False
                
                for path_template in DATAWEAVE_PATH_TEMPLATES:
                    dw_path = path_template.format(filename=zip_name)
                    payload = {"uris": [dw_path]}
                    
                    r = requests.post(API_URL, json=payload, headers=headers, timeout=15)
                    r.raise_for_status()
                    data = r.json()
                    
                    if data.get("code") != 0:
                        msg = data.get("msg", "")
                        if "Login required" in msg or data.get("code") == 401:
                            token_expired = True
                            break
                        continue
                    
                    url_data = data.get("data", {})
                    if isinstance(url_data, dict) and "urls" in url_data:
                        urls_list = url_data["urls"]
                        if urls_list and isinstance(urls_list[0], dict):
                            url = urls_list[0].get("url")
                            if url:
                                real_url = url
                                found_path = path_template.split("/")[-2]
                                break
                
                # Token è¿‡æœŸï¼Œå°è¯•åˆ·æ–°
                if token_expired:
                    if retry_token and attempt < max_retries - 1:
                        logger.critical("!!! Token å·²è¿‡æœŸ !!!")
                        new_token = self._get_dataweave_token(force_refresh=True)
                        headers = dict(headers)
                        headers["Authorization"] = new_token
                        continue
                    else:
                        return False
                
                if not real_url:
                    # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€é‡è¯•
                    return False
                
                logger.info(f"    æ‰¾åˆ°æ–‡ä»¶ï¼Œè·¯å¾„: {found_path}")
                
                # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ (å¢åŠ è¶…æ—¶)
                download_headers = {"User-Agent": headers["User-Agent"]}
                with requests.get(real_url, headers=download_headers, stream=True, timeout=600) as r:
                    r.raise_for_status()
                    total_size = int(r.headers.get('content-length', 0))
                    with open(temp_file, 'wb') as f:
                        downloaded = 0
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                if total_size > 0:
                                    percent = (downloaded / total_size) * 100
                                    sys.stdout.write(f"\r    ä¸‹è½½è¿›åº¦: {percent:.1f}%")
                                    sys.stdout.flush()
                    print()
                
                # éªŒè¯ä¸‹è½½å®Œæ•´æ€§
                if total_size > 0:
                    actual_size = temp_file.stat().st_size
                    if actual_size != total_size:
                        logger.error(f"    ä¸‹è½½ä¸å®Œæ•´: é¢„æœŸ {total_size} å­—èŠ‚ï¼Œå®é™… {actual_size} å­—èŠ‚")
                        if temp_file.exists():
                            temp_file.unlink()
                        if attempt < max_retries - 1:
                            logger.info(f"    é‡è¯•ä¸‹è½½ ({attempt + 2}/{max_retries})...")
                            time.sleep(2)
                            continue
                        return False
                
                # ä¸‹è½½å®Œæˆï¼Œé‡å‘½åä¸ºæ­£å¼æ–‡ä»¶
                if target_file.exists():
                    target_file.unlink()
                temp_file.rename(target_file)
                
                return True
                
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                logger.warning(f"    ç½‘ç»œé”™è¯¯: {e}")
                if temp_file.exists():
                    temp_file.unlink()
                if attempt < max_retries - 1:
                    logger.info(f"    é‡è¯•ä¸‹è½½ ({attempt + 2}/{max_retries})...")
                    time.sleep(2 * (attempt + 1))
                    continue
                return False
            except Exception as e:
                logger.error(f"    ä¸‹è½½å¤±è´¥: {e}")
                if temp_file.exists():
                    temp_file.unlink()
                return False
        
        return False
    
    # ==================== è¿è¡Œæµæ°´çº¿ ====================
    def run(self, steps: list = None):
        """è¿è¡Œæµæ°´çº¿"""
        all_steps = ['download', 'upload', 'process', 'check', 'move']
        
        if steps is None or 'all' in steps:
            steps = all_steps
        
        logger.info("=" * 60)
        logger.info("æ ‡æ³¨æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æµæ°´çº¿")
        logger.info(f"JSON ç›®å½•: {self.json_dir}")
        logger.info(f"æ‰§è¡Œæ­¥éª¤: {', '.join(steps)}")
        logger.info("=" * 60)
        
        try:
            if 'download' in steps:
                self.step1_download_zips()
            
            if 'upload' in steps:
                self.step2_upload_zips()
            
            if 'process' in steps:
                self.step3_process_on_server()
            
            if 'check' in steps:
                self.step4_check_annotations()
            
            if 'move' in steps:
                self.step5_move_to_final()
            
        finally:
            self._close_server()
        
        # è¾“å‡ºæ±‡æ€»
        self._print_summary()
    
    def _print_summary(self):
        """æ‰“å°æ‰§è¡Œæ±‡æ€»"""
        print()
        print("â•”" + "â•" * 50 + "â•—")
        print("â•‘  ğŸ“Š æ‰§è¡Œæ±‡æ€»".ljust(51) + "â•‘")
        print("â• " + "â•" * 50 + "â•£")
        
        stats = [
            ("â­ è·³è¿‡(å·²å­˜åœ¨)", len(self.results['skipped_server_exists'])),
            ("â¬‡ ä¸‹è½½æˆåŠŸ", len(self.results['downloaded'])),
            ("â¬† ä¸Šä¼ æˆåŠŸ", len(self.results['uploaded'])),
            ("âš™ å¤„ç†æˆåŠŸ", len(self.results['processed'])),
            ("âœ“ æ£€æŸ¥é€šè¿‡", len(self.results['check_passed'])),
            ("âœ— æ£€æŸ¥å¤±è´¥", len(self.results['check_failed'])),
            ("ğŸ“ å·²ç§»åŠ¨", len(self.results['moved_to_final'])),
        ]
        
        for label, count in stats:
            line = f"â•‘  {label}: {count}"
            print(line.ljust(51) + "â•‘")
        
        print("â•š" + "â•" * 50 + "â•")
        
        if self.results['check_failed']:
            print()
            print("  âš  æ£€æŸ¥æœªé€šè¿‡çš„æ•°æ®:")
            for name in self.results['check_failed']:
                report = self.local_check_dir / f"report_{name}.txt"
                print(f"    â€¢ {name}")
                if report.exists():
                    print(f"      æŠ¥å‘Š: {report}")
        
        # æ˜¾ç¤ºé”™è¯¯è¿½æº¯ä¿¡æ¯
        if self.errors:
            print()
            print("  âŒ å¤±è´¥è¯¦æƒ… (å¯è¿½æº¯):")
            for stem, error_list in self.errors.items():
                print(f"    â”Œâ”€ {stem}")
                for step, msg in error_list:
                    # æˆªæ–­è¿‡é•¿çš„é”™è¯¯ä¿¡æ¯
                    display_msg = msg[:80] + "..." if len(msg) > 80 else msg
                    print(f"    â”‚  [{step}] {display_msg}")
                print(f"    â””â”€")


def main():
    parser = argparse.ArgumentParser(description="æ ‡æ³¨æ•°æ®è‡ªåŠ¨åŒ–å¤„ç†æµæ°´çº¿")
    parser.add_argument('--json_dir', type=str, required=True,
                        help='æœ¬åœ° JSON æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--zip_dir', type=str, default=None,
                        help='æœ¬åœ° ZIP æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ä¸´æ—¶ç›®å½•)')
    parser.add_argument('--step', type=str, nargs='+', 
                        default=['all'],
                        choices=['all', 'download', 'upload', 'process', 'check', 'move'],
                        help='æ‰§è¡Œçš„æ­¥éª¤ (æ‰¹é‡æ¨¡å¼)')
    parser.add_argument('--streaming', '-s', action='store_true',
                        help='æµå¼å¤„ç†æ¨¡å¼: ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶å°±ç«‹å³å¤„ç†ï¼Œæ— éœ€ç­‰å¾…å…¨éƒ¨ä¸‹è½½å®Œæˆ')
    parser.add_argument('--parallel', '-p', action='store_true',
                        help='å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼: å¤šä¸ªæ–‡ä»¶åŒæ—¶ä¸‹è½½å’Œå¤„ç† (æ¨è)')
    parser.add_argument('--workers', '-w', type=int, default=None,
                        help=f'å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤ {MAX_WORKERS})')
    
    args = parser.parse_args()
    
    if not Path(args.json_dir).exists():
        logger.error(f"JSON ç›®å½•ä¸å­˜åœ¨: {args.json_dir}")
        return
    
    pipeline = AnnotationPipeline(args.json_dir, args.zip_dir)
    
    if args.parallel:
        # å¤šçº¿ç¨‹å¹¶è¡Œæ¨¡å¼
        pipeline.run_parallel(args.workers)
    elif args.streaming:
        # æµå¼å¤„ç†æ¨¡å¼
        pipeline.run_streaming()
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        pipeline.run(args.step)


if __name__ == "__main__":
    main()
