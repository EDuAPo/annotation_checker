# 标注数据自动化处理流水线

> 端到端的标注数据处理工具，支持从 DataWeave 下载、服务器处理、质量检查到最终部署的完整自动化流程。

## 📋 目录

- [功能特性](#-功能特性)
- [快速开始](#-快速开始)
- [处理模式](#-处理模式)
- [配置说明](#️-配置说明)
- [流程详解](#-流程详解)
- [检查规则](#-检查规则)
- [安全机制](#-安全机制)
- [错误追溯](#-错误追溯)
- [常见问题](#-常见问题)

---

## ✨ 功能特性

### 核心功能
- 🚀 **三种处理模式**: 批量模式、流式模式、并行模式
- 🔄 **自动登录**: 自动获取 DataWeave Token，无需手动更新
- 🔍 **多路径搜索**: 在 DataWeave 多个子目录中自动查找文件
- ⏭️ **智能跳过**: 自动跳过已处理的文件，支持断点续传
- 📊 **质量检查**: 服务器端自动检查标注质量
- 📈 **进度可视化**: 实时显示处理进度和状态

### 数据安全
- 🔒 **安全移动**: 目标存在时自动备份，不会覆盖删除
- 📝 **错误追溯**: 详细记录每个失败步骤的原因
- 🧹 **下载验证**: 使用临时文件 + ZIP 头验证，确保下载完整

---

## 🚀 快速开始

### 依赖安装

```bash
pip install paramiko requests pyyaml
```

### 基本用法

```bash
# 推荐：并行模式（最快）
python pipeline.py --json_dir /path/to/jsons --parallel

# 流式模式：下载一个处理一个
python pipeline.py --json_dir /path/to/jsons --streaming

# 批量模式：按步骤执行
python pipeline.py --json_dir /path/to/jsons --step all
```

### 命令行参数

| 参数 | 简写 | 说明 |
|------|------|------|
| `--json_dir` | | 本地 JSON 文件夹路径（必需）|
| `--zip_dir` | | 本地 ZIP 存储路径（可选）|
| `--parallel` | `-p` | 并行处理模式 |
| `--streaming` | `-s` | 流式处理模式 |
| `--workers` | `-w` | 并行线程数（默认 3）|
| `--step` | | 批量模式执行步骤 |

---

## 🔄 处理模式

### 1. 并行模式（推荐）

多个文件同时下载和处理，效率最高。

```bash
python pipeline.py --json_dir /path/to/jsons -p -w 4
```

**特点**:
- 使用 ThreadPoolExecutor 实现多线程
- 每个线程独立 SSH 连接
- 实时进度条显示
- 静默处理，完成后统一汇总

**输出示例**:
```
╔══════════════════════════════════════════════════╗
║  📦 标注数据自动化处理流水线 (并行模式)           ║
╚══════════════════════════════════════════════════╝
  📁 JSON目录: /path/to/jsons
  📋 共 50 个文件
  🔗 已连接服务器: 222.223.112.212
  📊 服务器状态: 10 ZIPs / 5 已完成
  🔑 Token 获取成功
  ⏭ 跳过已完成: 5 个
  📦 待处理: 45 个文件
  🧵 线程数: 4

[━━━━━━━━━━━╸─────────────] 20/45 (44%) │ ✓ scene_20250106_123456
```

### 2. 流式模式

下载一个文件后立即处理，适合边下载边处理。

```bash
python pipeline.py --json_dir /path/to/jsons -s
```

**特点**:
- 单文件完整流程
- 详细日志输出
- 适合调试和小批量处理

### 3. 批量模式

按步骤分阶段执行，适合需要手动干预的场景。

```bash
# 执行全部步骤
python pipeline.py --json_dir /path/to/jsons --step all

# 仅执行特定步骤
python pipeline.py --json_dir /path/to/jsons --step download upload
python pipeline.py --json_dir /path/to/jsons --step check move
```

**可用步骤**: `download`, `upload`, `process`, `check`, `move`

---

## ⚙️ 配置说明

编辑 `pipeline.py` 顶部的配置区域：

```python
# ================= 配置区域 =================

# DataWeave 登录凭据（推荐使用自动登录）
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"

# 多个可能的路径模板（按优先级顺序尝试）
DATAWEAVE_PATH_TEMPLATES = [
    "dataweave://my/TO_RERE/盲区数据/{filename}",
    "dataweave://my/TO_RERE/7Lidar_data/{filename}",
    # ... 更多路径
]

# 服务器配置
SERVER_IP = "222.223.112.212"
SERVER_USER = "user"
SERVER_ZIP_DIR = "/data01/rere_zips"           # ZIP 临时目录
SERVER_PROCESS_DIR = "/data01/processing"      # 处理中目录
SERVER_FINAL_DIR = "/data01/dataset/scenesnew" # 最终目录

# 处理完成后对原始 ZIP 的操作
ZIP_AFTER_PROCESS = "rename"  # rename / keep / delete

# 本地临时目录
LOCAL_TEMP_DIR = "/media/zgw/T7/test_pipeline_downzips/"

# 并行线程数
MAX_WORKERS = 3
```

### Token 获取

#### 方式一：自动登录（推荐）

配置用户名密码后，脚本自动登录获取最新 Token：

```python
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"
```

#### 方式二：手动配置

1. 登录 DataWeave 平台
2. 打开浏览器开发者工具 (F12)
3. 执行任意请求，在 Network 中找到 Authorization 头
4. 复制完整 Token 到 `AUTH_TOKEN`

---

## 📊 流程详解

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           标注数据自动化处理流水线                                    │
└─────────────────────────────────────────────────────────────────────────────────────┘

  ① download        ② upload          ③ process         ④ check           ⑤ move
 ┌────────────┐   ┌────────────┐   ┌────────────┐   ┌────────────┐   ┌────────────┐
 │ DataWeave  │──▶│ 上传ZIP到  │──▶│ 服务器解压 │──▶│ 服务器端   │──▶│ 移动到最终 │
 │ 下载ZIP    │   │ 服务器     │   │ 匹配JSON   │   │ 标注质检   │   │ 目录       │
 └────────────┘   └────────────┘   └────────────┘   └────────────┘   └────────────┘
       │                │                │                │                │
       ▼                ▼                ▼                ▼                ▼
    本地ZIP         服务器            处理后的          检查报告        最终存储
    (自动跳过     rere_zips          数据目录         (失败保留)      (检查通过)
    已存在的)
```

### 智能跳过

- **已完成的文件**: 检查 `SERVER_FINAL_DIR` 是否存在，存在则跳过
- **服务器已有 ZIP**: 检查 `SERVER_ZIP_DIR` 是否存在（含 `processed_` 前缀），跳过下载
- **本地已有 ZIP**: 验证 ZIP 头有效后跳过下载

### 处理逻辑

1. **下载**: 从 DataWeave 多个路径模板中查找并下载 ZIP
2. **上传**: 上传 ZIP 到服务器临时目录
3. **处理**: 解压 ZIP，匹配替换 JSON，调整目录结构
4. **检查**: 运行标注质量检查，生成报告
5. **移动**: 检查通过后移动到最终目录，清理本地文件

---

## 🔍 检查规则

标注质量检查基于 `rules_checker.py` 实现，支持以下检查项：

### 1. 尺寸范围检查

根据目标类别检查 3D 框的尺寸是否在合理范围内：

| 类别 | 长度 (m) | 宽度 (m) | 高度 (m) |
|------|----------|----------|----------|
| 车辆 (vehicle) | 2.0 - 12.0 | 1.0 - 3.0 | 1.0 - 4.0 |
| 行人 (pedestrian) | 0.3 - 1.2 | 0.3 - 1.2 | 1.0 - 2.5 |
| 锥桶 (cone) | 0.1 - 0.5 | 0.1 - 0.5 | 0.3 - 0.8 |
| 标志 (sign) | 0.05 - 2.0 | 0.05 - 2.0 | 0.05 - 3.0 |

### 2. 四元数归一化检查

检查标注的旋转四元数是否满足单位四元数条件：

```
||q|| = √(w² + x² + y² + z²) ≈ 1.0  (误差 < 0.01)
```

### 3. 车辆姿态角检查

检查车辆的 Roll（横滚）和 Pitch（俯仰）角是否异常：

- **Roll 角限制**: |roll| < 0.5 弧度 (≈28°)
- **Pitch 角限制**: |pitch| < 0.5 弧度 (≈28°)

正常行驶车辆不应有过大的横滚和俯仰，异常值可能表示标注错误。

### 4. 朝向与运动方向一致性检查 ⭐

这是最核心的检查项，用于检测车辆朝向标注是否正确。

#### 原理

```
                    运动方向 (motion_yaw)
                         ↗
                        /
                   ┌───────┐
                   │  车辆  │→ 标注朝向 (obj_yaw)
                   └───────┘

如果 motion_yaw ≠ obj_yaw (且非倒车)，则标注可能错误
```

#### 判定逻辑

| 角度差 | 状态 | 结果 |
|--------|------|------|
| `diff < 60°` | 正常前进 | ✅ 通过 |
| `|diff - 180°| < 60°` | 正常倒车 | ✅ 通过 |
| 其他 | 朝向与运动不一致 | ❌ 报错 |

#### 过滤条件

- 只检查 `vehicle` 类型目标
- 位移 < 0.5 米视为静止，不检查（避免噪声干扰）
- 需要前后帧数据才能计算运动方向

### 5. 自车位姿补偿 🆕

由于标注数据中的坐标是**自车坐标系**下的位置，而自车每帧都在移动，因此需要使用 `ins.json` 中的自车位姿信息将目标位置转换到**世界坐标系 (UTM)** 后再计算运动方向。

#### 工作原理

```
自车坐标系位置 (translation)    自车位姿 (ins.json)
        │                            │
        ▼                            ▼
    ┌───────────────────────────────────┐
    │  世界坐标系位置 = R_ego × pos + T_ego │
    └───────────────────────────────────┘
                    │
                    ▼
          计算目标真实运动方向
                    │
                    ▼
         与标注朝向对比（转换后）
```

#### ins.json 使用的字段

| 字段 | 说明 |
|------|------|
| `utm_x, utm_y, utm_z` | 自车在 UTM 坐标系中的位置 |
| `quaternion_w/x/y/z` | 自车的姿态四元数 |
| `azimuth` | 自车航向角（用于转换标注朝向） |

#### 启用条件

- 自动检测 `ins.json` 文件
- 文件存在时自动启用位姿补偿
- 文件不存在时回退到直接计算模式

#### 示例报告

```
检查报告 - scene_20250106_123456
==================================================

统计汇总:
  总帧数: 100
  总对象数: 523
  问题帧数: 3
  问题对象数: 5
  通过率: 97.0%
  自车位姿补偿: 已启用 (100 条INS数据)

==================================================

问题详情:

帧: 25
  对象: token_25_1-5 (类别: vehicle.car)
    - 朝向与运动方向不一致: 差值87.3度
```

### 配置文件

检查规则可在 `configs/default.yaml` 中配置：

```yaml
rules:
  min_lidar_points: 3                # 最小点云数（暂禁用）
  vehicle:
    length_range: [2.0, 12.0]
    width_range: [1.0, 3.0]
    height_range: [1.0, 4.0]
  pedestrian:
    length_range: [0.3, 1.2]
    width_range: [0.3, 1.2]
    height_range: [1.0, 2.5]
  cone:
    length_range: [0.1, 0.5]
    width_range: [0.1, 0.5]
    height_range: [0.3, 0.8]
```

---

## 🔒 安全机制

### 数据不丢失

1. **安全移动**: 移动前检查目标是否存在，存在则备份为 `xxx.bak.时间戳`
   ```python
   # 不再使用危险的 rm -rf
   # 而是先备份再移动
   backup_dst = f"{dst}.bak.{datetime.now().strftime('%Y%m%d%H%M%S')}"
   ssh.exec_command(f"mv '{dst}' '{backup_dst}'")
   ```

2. **下载验证**: 
   - 使用 `.tmp` 临时文件，下载完成后重命名
   - 验证文件大小和 ZIP 头 (`PK`)
   - 不完整的下载会在启动时自动清理

3. **检查通过才移动**: 只有标注质量检查通过的数据才会移动到最终目录

---

## 🔍 错误追溯

所有失败都会记录详细原因，方便追溯：

```
╔══════════════════════════════════════════════════╗
║  📊 执行汇总                                      ║
╠══════════════════════════════════════════════════╣
║  ⏭ 跳过(已存在): 5                               ║
║  ⬇ 下载成功: 40                                  ║
║  ⬆ 上传成功: 40                                  ║
║  ⚙ 处理成功: 38                                  ║
║  ✓ 检查通过: 35                                  ║
║  ✗ 检查失败: 3                                   ║
║  📁 已移动: 35                                   ║
╚══════════════════════════════════════════════════╝

  ⚠ 检查未通过的数据:
    • scene_20250106_123456
      报告: /path/to/check_data/report_scene_20250106_123456.txt

  ❌ 失败详情 (可追溯):
    ┌─ scene_20250106_789012
    │  [下载] 下载失败，文件在DataWeave中不存在或网络问题
    └─
    ┌─ scene_20250106_345678
    │  [服务器处理] 处理脚本返回错误码 1: FileNotFoundError...
    └─
```

### 错误类型

| 步骤 | 常见错误 | 解决方案 |
|------|----------|----------|
| 下载 | 文件不存在 | 检查 DataWeave 路径模板 |
| 下载 | Token 过期 | 重新运行（自动刷新）|
| 上传 | SSH 连接失败 | 检查网络和 SSH 配置 |
| 处理 | 脚本执行失败 | 查看服务器 `/tmp/zip_worker.py` |
| 检查 | 发现问题帧 | 查看本地报告文件 |
| 移动 | 权限不足 | 检查目标目录权限 |

---

## ❓ 常见问题

### Q: Token 过期怎么办？

**A**: 配置自动登录后会自动刷新：
```python
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"
```

### Q: 如何断点续传？

**A**: 直接重新运行脚本即可，会自动跳过：
- 服务器最终目录已存在的数据
- 服务器 ZIP 目录已存在的文件
- 本地已下载完整的 ZIP

### Q: 检查失败的数据怎么处理？

**A**: 
1. 查看本地报告 `check_data/report_xxx.txt`
2. 修复标注问题后重新上传 JSON
3. 删除服务器处理目录中的对应文件夹
4. 重新运行流水线

### Q: 如何添加新的 DataWeave 路径？

**A**: 在 `DATAWEAVE_PATH_TEMPLATES` 中添加：
```python
DATAWEAVE_PATH_TEMPLATES = [
    "dataweave://my/TO_RERE/盲区数据/{filename}",
    "dataweave://my/TO_RERE/新目录/{filename}",  # 添加新路径
    # ...
]
```

### Q: 并行模式线程数设置多少合适？

**A**: 
- 推荐 2-4 个线程
- 太多会占用服务器资源
- 网络好时可适当增加

```bash
python pipeline.py --json_dir /path -p -w 4
```

---

## 📁 项目结构

```
annotation_checker/
├── src/
│   ├── pipeline.py        # 主控脚本（核心）
│   ├── rules_checker.py   # 标注规则检查器
│   ├── visualizer.py      # 可视化工具
│   └── ...
├── configs/
│   ├── default.yaml       # 默认配置
│   └── user_config.yaml   # 用户配置
└── reports/
    └── ...                # 检查报告输出
```

---

## 📝 更新日志

### v2.1 (2026-01-07)
- ✅ 新增自车位姿补偿功能（使用 ins.json）
- ✅ 优化朝向一致性检查算法
- ✅ 检查报告显示是否启用位姿补偿
- ✅ Token 自动刷新机制（50分钟有效期内自动刷新）
- ✅ SSH 连接断线自动重连
- ✅ 下载失败自动重试（最多3次）

### v2.0 (2026-01-06)
- ✅ 新增并行处理模式（多线程）
- ✅ 新增流式处理模式
- ✅ 自动登录获取 Token
- ✅ 多路径搜索功能
- ✅ 安全移动（备份而非删除）
- ✅ 错误追溯系统
- ✅ 进度可视化
- ✅ 下载完整性验证

### v1.0
- 基础批量处理功能
- 单一路径下载

---


