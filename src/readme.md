# 标注数据自动化处理流水线

> 端到端的标注数据处理工具，支持从 DataWeave 下载、服务器处理、质量检查到最终部署的完整自动化流程。

## 📋 目录

- [功能特性](#-功能特性)
- [快速开始](#-快速开始)
- [处理模式](#-处理模式)
  - [单步执行命令](#单步执行命令)
- [配置说明](#️-配置说明)
- [流程详解](#-流程详解)
- [检查规则](#-检查规则)
- [安全机制](#-安全机制)
- [错误追溯](#-错误追溯)
- [常见问题](#-常见问题)

---

## ✨ 功能特性

### 核心功能
- 🚀 **三种处理模式**: 批量模式、流式模式、并行模式
- 🔄 **自动登录**: 自动获取 DataWeave Token，无需手动更新
- 🔍 **多路径搜索**: 在 DataWeave 多个子目录中自动查找文件
- ⏭️ **智能跳过**: 自动跳过已处理的文件，支持断点续传
- 📊 **质量检查**: 服务器端自动检查标注质量
- 📈 **进度可视化**: 实时显示处理进度和状态
- 🔗 **飞书表格追踪**: 自动将处理情况同步到飞书多维表格

### 数据安全
- 🔒 **安全移动**: 目标存在时自动备份，不会覆盖删除
- 📝 **错误追溯**: 详细记录每个失败步骤的原因
- 🧹 **下载验证**: 使用临时文件 + ZIP 头验证，确保下载完整
- 🗑️ **智能清理**: 数据处理完成后自动清理本地临时文件
- 📊 **完整统计**: 所有数据（检查通过/失败/跳过）都统计关键帧数量
- 🔄 **多级fallback**: 服务器失败时自动从本地JSON获取关键帧数量

---

## 🚀 快速开始

### 依赖安装

```bash
pip install paramiko requests pyyaml
```

### 基本用法

```bash
# 推荐：并行模式（最快）
python pipeline.py --json_dir /path/to/jsons --parallel

# 流式模式：下载一个处理一个
python pipeline.py --json_dir /path/to/jsons --streaming

# 批量模式：按步骤执行
python pipeline.py --json_dir /path/to/jsons --step all
```

### 命令行参数

| 参数 | 简写 | 说明 |
|------|------|------|
| `--json_dir` | | 本地 JSON 文件夹路径（必需）|
| `--zip_dir` | | 本地 ZIP 存储路径（可选）|
| `--parallel` | `-p` | 并行处理模式 |
| `--streaming` | `-s` | 流式处理模式 |
| `--workers` | `-w` | 并行线程数（默认 3）|
| `--step` | | 批量模式执行步骤 |

---

## 🔄 处理模式

### 1. 并行模式（推荐）

多个文件同时下载和处理，效率最高。

```bash
python pipeline.py --json_dir /path/to/jsons -p -w 4
```

**特点**:
- 使用 ThreadPoolExecutor 实现多线程
- 每个线程独立 SSH 连接
- 实时进度条显示
- 静默处理，完成后统一汇总

**输出示例**:
```
╔══════════════════════════════════════════════════╗
║  📦 标注数据自动化处理流水线 (并行模式)           ║
╚══════════════════════════════════════════════════╝
  📁 JSON目录: /path/to/jsons
  📋 共 50 个文件
  🔗 已连接服务器: 222.223.112.212
  📊 服务器状态: 10 ZIPs / 5 已完成
  🔑 Token 获取成功
  ⏭ 跳过已完成: 5 个
  📦 待处理: 45 个文件
  🧵 线程数: 4

[━━━━━━━━━━━╸─────────────] 20/45 (44%) │ ✓ scene_20250106_123456
```

### 2. 流式模式

下载一个文件后立即处理，适合边下载边处理。

```bash
python pipeline.py --json_dir /path/to/jsons -s
```

**特点**:
- 单文件完整流程
- 详细日志输出
- 适合调试和小批量处理

### 3. 批量模式

按步骤分阶段执行，适合需要手动干预或只执行部分流程的场景。

```bash
# 执行全部步骤
python pipeline.py --json_dir /path/to/jsons --step all

# 仅执行特定步骤
python pipeline.py --json_dir /path/to/jsons --step download upload
python pipeline.py --json_dir /path/to/jsons --step check move
```

**可用步骤**:
- `download`: 步骤1 - 从 DataWeave 下载 ZIP 文件
- `upload`: 步骤2 - 上传 ZIP 文件到服务器
- `process`: 步骤3 - 在服务器上解压和处理数据
- `check`: 步骤4 - 检查标注质量
- `move`: 步骤5 - 将检查通过的数据移动到最终目录

#### 单步执行命令

如果需要单独执行某个步骤，可以使用以下命令：

```bash
# 步骤1: 下载ZIP文件
python pipeline.py --json_dir /path/to/jsons --step download

# 步骤2: 上传ZIP文件到服务器
python pipeline.py --json_dir /path/to/jsons --step upload

# 步骤3: 在服务器上处理数据
python pipeline.py --json_dir /path/to/jsons --step process

# 步骤4: 检查标注质量
python pipeline.py --json_dir /path/to/jsons --step check

# 步骤5: 移动到最终目录
python pipeline.py --json_dir /path/to/jsons --step move
```

#### 完整流水线运行

```bash
# 完整流水线（推荐生产环境使用）
python pipeline.py --json_dir /path/to/jsons --step all
```

---

## ⚙️ 配置说明

编辑 `pipeline.py` 顶部的配置区域：

```python
# ================= 配置区域 =================

# DataWeave 登录凭据（推荐使用自动登录）
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"

# 多个可能的路径模板（按优先级顺序尝试）
DATAWEAVE_PATH_TEMPLATES = [
    "dataweave://my/TO_RERE/盲区数据/{filename}",
    "dataweave://my/TO_RERE/7Lidar_data/{filename}",
    # ... 更多路径
]

# 服务器配置
SERVER_IP = "222.223.112.212"
SERVER_USER = "user"
SERVER_ZIP_DIR = "/data01/rere_zips"           # ZIP 临时目录
SERVER_PROCESS_DIR = "/data01/processing"      # 处理中目录
SERVER_FINAL_DIR = "/data01/dataset/scenesnew" # 最终目录

# 处理完成后对原始 ZIP 的操作
ZIP_AFTER_PROCESS = "rename"  # rename / keep / delete

# 本地临时目录
LOCAL_TEMP_DIR = "/media/zgw/T7/test_pipeline_downzips/"

# 并行线程数
MAX_WORKERS = 3
```

### Token 获取

#### 方式一：自动登录（推荐）

配置用户名密码后，脚本自动登录获取最新 Token：

```python
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"
```

#### 方式二：手动配置

1. 登录 DataWeave 平台
2. 打开浏览器开发者工具 (F12)
3. 执行任意请求，在 Network 中找到 Authorization 头
4. 复制完整 Token 到 `AUTH_TOKEN`

---

## 📊 流程详解

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           标注数据自动化处理流水线                                    │
└─────────────────────────────────────────────────────────────────────────────────────┘

  ① download        ② upload          ③ process         ④ check           ⑤ move
 ┌────────────┐   ┌────────────┐   ┌────────────┐   ┌────────────┐   ┌────────────┐
 │ DataWeave  │──▶│ 上传ZIP到  │──▶│ 服务器解压 │──▶│ 服务器端   │──▶│ 移动到最终 │
 │ 下载ZIP    │   │ 服务器     │   │ 匹配JSON   │   │ 标注质检   │   │ 目录       │
 └────────────┘   └────────────┘   └────────────┘   └────────────┘   └────────────┘
       │                │                │                │                │
       ▼                ▼                ▼                ▼                ▼
    本地ZIP         服务器            处理后的          检查报告        最终存储
    (自动跳过     rere_zips          数据目录         (失败保留)      (检查通过)
    已存在的)
```

### 智能跳过

- **已完成的文件**: 检查 `SERVER_FINAL_DIR` 是否存在，存在则跳过
- **服务器已有 ZIP**: 检查 `SERVER_ZIP_DIR` 是否存在（含 `processed_` 前缀），跳过下载
- **本地已有 ZIP**: 验证 ZIP 头有效后跳过下载

### 处理逻辑

1. **下载**: 从 DataWeave 多个路径模板中查找并下载 ZIP
2. **上传**: 上传 ZIP 到服务器临时目录
3. **处理**: 解压 ZIP，匹配替换 JSON，调整目录结构
4. **检查**: 运行标注质量检查，生成报告
5. **移动**: 检查通过后移动到最终目录，清理本地文件

---

## 🔍 检查规则

标注质量检查基于 `rules_checker.py` 实现，支持以下检查项：

### 1. 尺寸范围检查

根据目标类别检查 3D 框的尺寸是否在合理范围内：

| 类别 | 长度 (m) | 宽度 (m) | 高度 (m) |
|------|----------|----------|----------|
| 车辆 (vehicle) | 2.0 - 12.0 | 1.0 - 3.0 | 1.0 - 4.0 |
| 行人 (pedestrian) | 0.3 - 1.2 | 0.3 - 1.2 | 1.0 - 2.5 |
| 锥桶 (cone) | 0.1 - 0.5 | 0.1 - 0.5 | 0.3 - 0.8 |
| 标志 (sign) | 0.05 - 2.0 | 0.05 - 2.0 | 0.05 - 3.0 |

### 2. 四元数归一化检查

检查标注的旋转四元数是否满足单位四元数条件：

```
||q|| = √(w² + x² + y² + z²) ≈ 1.0  (误差 < 0.01)
```

### 3. 车辆姿态角检查

检查车辆的 Roll（横滚）和 Pitch（俯仰）角是否异常：

- **Roll 角限制**: |roll| < 0.5 弧度 (≈28°)
- **Pitch 角限制**: |pitch| < 0.5 弧度 (≈28°)

正常行驶车辆不应有过大的横滚和俯仰，异常值可能表示标注错误。

### 4. 朝向与运动方向一致性检查 ⭐

这是最核心的检查项，用于检测车辆朝向标注是否正确。

#### 原理

```
                    运动方向 (motion_yaw)
                         ↗
                        /
                   ┌───────┐
                   │  车辆  │→ 标注朝向 (obj_yaw)
                   └───────┘

如果 motion_yaw ≠ obj_yaw (且非倒车)，则标注可能错误
```

#### 判定逻辑

| 角度差 | 状态 | 结果 |
|--------|------|------|
| `diff < 60°` | 正常前进 | ✅ 通过 |
| `|diff - 180°| < 60°` | 正常倒车 | ✅ 通过 |
| 其他 | 朝向与运动不一致 | ❌ 报错 |

#### 过滤条件

- 只检查 `vehicle` 类型目标
- 位移 < 0.5 米视为静止，不检查（避免噪声干扰）
- 需要前后帧数据才能计算运动方向

### 5. 自车位姿补偿 🆕

由于标注数据中的坐标是**自车坐标系**下的位置，而自车每帧都在移动，因此需要使用 `ins.json` 中的自车位姿信息将目标位置转换到**世界坐标系 (UTM)** 后再计算运动方向。

#### 工作原理

```
自车坐标系位置 (translation)    自车位姿 (ins.json)
        │                            │
        ▼                            ▼
    ┌───────────────────────────────────┐
    │  世界坐标系位置 = R_ego × pos + T_ego │
    └───────────────────────────────────┘
                    │
                    ▼
          计算目标真实运动方向
                    │
                    ▼
         与标注朝向对比（转换后）
```

#### ins.json 使用的字段

| 字段 | 说明 |
|------|------|
| `utm_x, utm_y, utm_z` | 自车在 UTM 坐标系中的位置 |
| `quaternion_w/x/y/z` | 自车的姿态四元数 |
| `azimuth` | 自车航向角（用于转换标注朝向） |

#### 启用条件

- 自动检测 `ins.json` 文件
- 文件存在时自动启用位姿补偿
- 文件不存在时回退到直接计算模式

#### 示例报告

```
检查报告 - scene_20250106_123456
==================================================

统计汇总:
  总帧数: 100
  总对象数: 523
  问题帧数: 3
  问题对象数: 5
  通过率: 97.0%
  自车位姿补偿: 已启用 (100 条INS数据)

==================================================

问题详情:

帧: 25
  对象: token_25_1-5 (类别: vehicle.car)
    - 朝向与运动方向不一致: 差值87.3度
```

### 配置文件

检查规则可在 `configs/default.yaml` 中配置：

```yaml
rules:
  min_lidar_points: 3                # 最小点云数（暂禁用）
  vehicle:
    length_range: [2.0, 12.0]
    width_range: [1.0, 3.0]
    height_range: [1.0, 4.0]
  pedestrian:
    length_range: [0.3, 1.2]
    width_range: [0.3, 1.2]
    height_range: [1.0, 2.5]
  cone:
    length_range: [0.1, 0.5]
    width_range: [0.1, 0.5]
    height_range: [0.3, 0.8]
```

---

## 🔒 安全机制

### 数据不丢失

1. **安全移动**: 移动前检查目标是否存在，存在则备份为 `xxx.bak.时间戳`
   ```python
   # 不再使用危险的 rm -rf
   # 而是先备份再移动
   backup_dst = f"{dst}.bak.{datetime.now().strftime('%Y%m%d%H%M%S')}"
   ssh.exec_command(f"mv '{dst}' '{backup_dst}'")
   ```

2. **下载验证**: 
   - 使用 `.tmp` 临时文件，下载完成后重命名
   - 验证文件大小和 ZIP 头 (`PK`)
   - 不完整的下载会在启动时自动清理

3. **检查通过才移动**: 只有标注质量检查通过的数据才会移动到最终目录

---

## 🔍 错误追溯

所有失败都会记录详细原因，方便追溯：

```
╔══════════════════════════════════════════════════╗
║  📊 执行汇总                                      ║
╠══════════════════════════════════════════════════╣
║  ⏭ 跳过(已存在): 5                               ║
║  ⬇ 下载成功: 40                                  ║
║  ⬆ 上传成功: 40                                  ║
║  ⚙ 处理成功: 38                                  ║
║  ✓ 检查通过: 35                                  ║
║  ✗ 检查失败: 3                                   ║
║  📁 已移动: 35                                   ║
╚══════════════════════════════════════════════════╝

  ⚠ 检查未通过的数据:
    • scene_20250106_123456
      报告: /path/to/check_data/report_scene_20250106_123456.txt

  ❌ 失败详情 (可追溯):
    ┌─ scene_20250106_789012
    │  [下载] 下载失败，文件在DataWeave中不存在或网络问题
    └─
    ┌─ scene_20250106_345678
    │  [服务器处理] 处理脚本返回错误码 1: FileNotFoundError...
    └─
```

### 错误类型

| 步骤 | 常见错误 | 解决方案 |
|------|----------|----------|
| 下载 | 文件不存在 | 检查 DataWeave 路径模板 |
| 下载 | Token 过期 | 重新运行（自动刷新）|
| 上传 | SSH 连接失败 | 检查网络和 SSH 配置 |
| 处理 | 脚本执行失败 | 查看服务器 `/tmp/zip_worker.py` |
| 检查 | 发现问题帧 | 查看本地报告文件 |
| 移动 | 权限不足 | 检查目标目录权限 |

---

## ❓ 常见问题

### Q: Token 过期怎么办？

**A**: 配置自动登录后会自动刷新：
```python
DATAWEAVE_USERNAME = "your_email@example.com"
DATAWEAVE_PASSWORD = "your_password"
```

### Q: 如何断点续传？

**A**: 直接重新运行脚本即可，会自动跳过：
- 服务器最终目录已存在的数据
- 服务器 ZIP 目录已存在的文件
- 本地已下载完整的 ZIP

**飞书表格数据完整性保证：**
- 已处理数据会更新现有记录，不会创建重复记录
- 新数据会智能分配序号，填补表格空行
- 支持部分数据重新处理，不影响其他数据的记录

### Q: 检查失败的数据怎么处理？

**A**: 
1. 查看本地报告 `check_data/report_xxx.txt`
2. 修复标注问题后重新上传 JSON
3. 删除服务器处理目录中的对应文件夹
4. 重新运行流水线

### Q: 本地ZIP文件什么时候会被删除？

**A**: 每个数据完成完整流水线（检查通过 + 移动到最终目录）后会立即删除本地ZIP文件：
- ✅ **检查通过**的数据：处理完成后立即删除
- ❌ **检查失败**的数据：保留ZIP文件用于调试
- ❌ **处理失败**的数据：保留ZIP文件用于重试

### Q: 飞书表格会统计所有数据的关键帧数量吗？

**A**: 是的，所有经过流水线处理的数据都会统计关键帧数量并更新到飞书表格：
- ✅ **检查通过**的数据：统计关键帧数量
- ✅ **检查失败**的数据：同样统计关键帧数量
- ✅ **处理失败**的数据：如果已部分解压，也会尝试统计关键帧数量
- ✅ **已完成**的数据：重新统计关键帧数量
- ✅ **跳过处理**的数据：统计已存在数据的关键帧数量

### Q: 如何添加新的 DataWeave 路径？

**A**: 在 `DATAWEAVE_PATH_TEMPLATES` 中添加：
```python
DATAWEAVE_PATH_TEMPLATES = [
    "dataweave://my/TO_RERE/盲区数据/{filename}",
    "dataweave://my/TO_RERE/新目录/{filename}",  # 添加新路径
    # ...
]
```

### Q: 并行模式线程数设置多少合适？

**A**: 
- 推荐 2-4 个线程
- 太多会占用服务器资源
- 网络好时可适当增加

```bash
python pipeline.py --json_dir /path -p -w 4
```

---

## 📁 项目结构

```
annotation_checker/
├── src/
│   ├── pipeline.py        # 主控脚本（核心）
│   ├── rules_checker.py   # 标注规则检查器
│   ├── visualizer.py      # 可视化工具
│   └── ...
├── configs/
│   ├── default.yaml       # 默认配置
│   └── user_config.yaml   # 用户配置
└── reports/
    └── ...                # 检查报告输出
```

---

## 📝 更新日志

### v2.3 (2026-01-07)
- ✅ 优化飞书表格序号分配逻辑，支持填补空行
- ✅ 改进本地ZIP文件清理策略，单个数据完成时立即删除
- ✅ 新增飞书表格分页获取功能，支持大量数据处理
- ✅ 完善多线程模式下的日志输出
- ✅ 强化断点续传逻辑，保证飞书表格数据不重复不缺失
- ✅ 全面修复关键帧统计逻辑，所有经过流水线的数据都统计关键帧数量（包括检查失败、处理失败、跳过处理的数据）
- ✅ 新增智能去重机制，避免重复写入已存在的关键帧数量

### v2.2 (2026-01-07)
- ✅ 新增飞书多维表格追踪功能
- ✅ 自动识别数据属性（拉框、线段、贴边、盲区等）
- ✅ 处理完成后自动更新飞书表格记录

### v2.1 (2026-01-07)
- ✅ 新增自车位姿补偿功能（使用 ins.json）
- ✅ 优化朝向一致性检查算法
- ✅ 检查报告显示是否启用位姿补偿
- ✅ Token 自动刷新机制（50分钟有效期内自动刷新）
- ✅ SSH 连接断线自动重连
- ✅ 下载失败自动重试（最多3次）

### v2.0 (2026-01-06)
- ✅ 新增并行处理模式（多线程）
- ✅ 新增流式处理模式
- ✅ 自动登录获取 Token
- ✅ 多路径搜索功能
- ✅ 安全移动（备份而非删除）
- ✅ 错误追溯系统
- ✅ 进度可视化
- ✅ 下载完整性验证

### v1.0
- 基础批量处理功能
- 单一路径下载

---

## 🔗 飞书表格追踪

### 功能说明

Pipeline 支持将数据处理情况自动同步到飞书多维表格，方便团队追踪数据处理进度。

**智能序号分配：**
- 📊 **顺序填写**: 新数据按序号顺序填补空行，确保表格连续
- 🔄 **断点续传**: 已存在的记录保持原有序号，新数据填补空缺
- �️ **数据完整性**: 保证断点续传时无重复记录、无缺失记录
- �📝 **自动更新**: 处理完成后自动更新记录状态和时间戳- 📈 **完整统计**: 所有数据（检查通过/失败）都统计关键帧数量- 🔍 **智能去重**: 已存在关键帧数量的记录不会被重复写入，避免数据覆盖
**自动检测的数据属性：**
- **拉框**: 路径包含"拉框"、"框标注"、"box"、"bbox"
- **线段**: 路径包含"线段"、"line"、"polyline"、"划线"
- **贴边**: 路径包含"贴边"、"edge"、"边缘"、"boundary"
- **盲区**: 路径包含"盲区"、"blind"、"遮挡"
- **关键帧**: 路径包含"关键帧"、"keyframe"、"key_frame"

### 配置步骤

1. **创建飞书应用**

   在 [飞书开放平台](https://open.feishu.cn/) 创建应用，获取 App ID 和 App Secret。

2. **开启权限**

   在应用中开启以下权限：
   - `bitable:app` - 多维表格读写权限

3. **设置表格协作者**

   将创建的应用添加为飞书表格的协作者（需要编辑权限）。

4. **配置文件**

   编辑 `configs/feishu_config.yaml`：

   ```yaml
   # 飞书应用凭证
   app_id: "cli_xxxxx"
   app_secret: "xxxxx"

   # 多维表格配置
   app_token: "bascxxxxx"  # 从表格 URL 获取
   table_id: "tblxxxxx"    # 数据表 ID
   ```

5. **获取表格 ID**

   使用命令行工具获取表格信息：

   ```bash
   # 测试连接
   python src/feishu_tracker.py --test

   # 列出所有数据表
   python src/feishu_tracker.py --list-tables

   # 列出表格字段
   python src/feishu_tracker.py --list-fields
   ```

### 表格字段要求

飞书表格需要包含以下字段（列）：

| 字段名 | 类型 | 说明 |
|--------|------|------|
| 名称 | 文本 | 数据名称 |
| 拉框 | 复选框 | 拉框属性标记 |
| 线段 | 复选框 | 线段属性标记 |
| 贴边 | 复选框 | 贴边属性标记 |
| 盲区 | 复选框 | 盲区属性标记 |
| 更新时间 | 文本/日期 | 最后更新时间 |

### 使用示例

```bash
# 运行时自动检测属性并更新表格
python pipeline.py --json_dir "/media/zgw/T7/1.6线拉框导出/盲区数据/" --step check move

# 上述命令会：
# 1. 自动检测到属性：拉框、盲区
# 2. 处理完成后，在飞书表格中记录每个数据名称
# 3. 在对应的"拉框"和"盲区"列打勾
```

### 执行汇总示例

```
╔══════════════════════════════════════════════════╗
║  📊 执行汇总                                      ║
╠══════════════════════════════════════════════════╣
║  ✓ 检查通过: 15                                   ║
║  📁 已移动: 15                                    ║
╚══════════════════════════════════════════════════╝

  🔗 飞书表格追踪:
    • 检测属性: 拉框, 盲区
    • 新增记录: 10
    • 更新记录: 5
```

---

## ❓ 常见问题

### Q: 为什么有些文件没有关键帧数量统计？

**A**: 脚本实现了多级fallback机制，确保所有文件都能获取关键帧数量：

1. **跳过文件**: 从服务器最终目录的 `sample.json` 获取
2. **处理成功文件**: 从服务器处理目录的 `sample.json` 获取  
3. **处理失败文件**: 按以下优先级获取：
   - 服务器最终目录（如果之前处理过）
   - 服务器处理目录（如果部分解压）
   - 本地JSON文件（统计JSON中的帧数量）

如果仍然获取失败，会在日志中显示警告信息。

### Q: 并行模式和流式模式有什么区别？

**A**: 
- **并行模式**: 多个文件同时处理，效率最高，适合大批量处理
- **流式模式**: 下载一个处理一个，详细日志输出，适合调试

### Q: 如何处理已存在的文件？

**A**: 脚本会自动检测并跳过已处理的文件：
- 检查服务器最终目录是否存在对应文件夹
- 已存在的文件会显示为"跳过已完成"，但仍会统计关键帧数量

### Q: 飞书表格中没有显示关键帧数量？

**A**: 请确保飞书表格包含"关键帧数"字段，脚本会自动将关键帧数量写入该字段。

---

## 📝 更新日志

### v2.1.0 (2026-01-08)
- ✨ **关键帧统计完善**: 实现多级fallback，确保所有文件（跳过/失败/成功）都能获取关键帧数量
- 🔧 **并行模式优化**: 修复跳过文件关键帧统计问题
- 📊 **统计完整性**: 处理失败的文件现在也能正确统计关键帧数量
- 🐛 **错误处理改进**: 增强关键帧获取失败时的日志输出

### v2.0.0 (2026-01-XX)
- 🚀 **并行处理模式**: 新增多线程并行处理，大幅提升处理效率
- 🔄 **飞书表格集成**: 自动同步处理状态到飞书多维表格
- 📈 **进度可视化**: 实时进度条和详细统计信息
- 🛡️ **安全机制**: 目标存在时自动备份，防止数据丢失

